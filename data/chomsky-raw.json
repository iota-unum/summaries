[
  {
    "tag": "h2",
    "header": "1 Linguistic contributions to the study of mind: past",
    "content": [
      "The study of language has long been seen as a way to gain insight into human nature. From scholars and amateurs to modern linguists, philosophers, and psychologists, the relationship between language and the mind has been explored. In recent years, technological advances and increasing interdisciplinary collaboration have led to increased skepticism about established theories and approaches, as well as a search for new ways of approaching old questions. However, many of the approaches used have been found to be inadequate, and it has become increasingly clear that a qualitative rather than quantitative approach is needed to understand the nature of linguistic competence. As such, there has been a renewed interest in classical questions related to language and mind, and a hope that by revisiting these topics, new insights can be gained and applied to current research.\n\n",
      "Huarte postulated two types of intelligence: the docile wit, which is limited to the data of sense, and normal human intelligence, which is capable of independently constructing cognitive systems and generating new thoughts. He also postulated a third type of intelligence, \"true creativity,\" which involves a mixture of madness.\n\n",
      "Language is a species-specific human possession and is innovative, free from stimulus control, and appropriate and coherent. It serves as an instrument of thought and self-expression, and is potentially infinite in scope. Descartes argued that it was beyond the bounds of mechanical explanation, and various thinkers developed a general theory of linguistic structure.\n\n",
      "Philosophical grammar recognizes the importance of phrase analysis and deep structure related to surface structure by mental operations. It is based on the notion that language is a relation between sound and meaning. Ellipsis is used as a device for the interpretation of texts. The theory holds that both the deep structure and the surface structure are “present to the mind” when a sentence is produced or understood. A grammar is represented in the mind to characterize and associate deep and surface structures.\n\n",
      "The Port-Royal theory of deep and surface structure belongs to psychology as an attempt to elaborate Huarte’s second type of wit, as an exploration of the properties of normal human intelligence. Modern structural linguistics restricts itself to the analysis of what is called surface structure, to formal properties that are explicit in the signal and to phrases and units that can be determined from the signal by techniques of segmentation and classification. The concept of ellipsis in Sanctius is one of many techniques, to be applied as conditions warrant and having no necessary mental representation as an aspect of a normal intelligence. Saussure expressed the view that processes of sentence formation do not belong to the system of language at all, that syntax is a trivial matter, and that sentence formation is a process of free creation, unconstrained by linguistic rule. Whitney argued that there is nothing universal about the form of language and that one can learn nothing about the general properties of human intelligence from the study of the arbitrary agglomeration of forms that constitutes a human language. Saussure and others made substantial progress in modern structural-descriptive linguistics.\n\n"
    ],
    "wordCount": 9982
  },
  {
    "tag": "h2",
    "header": "2 Linguistic contributions to the study of mind: present",
    "content": [
      "Knowledge of a language is a system of rules that relate sound and meaning. The linguist constructing a grammar is proposing a hypothesis concerning this internalized system, with certain empirical consequences. The search for explanatory theories must begin with an attempt to determine these systems of rules and reveal the principles that govern them. Universal grammar consists of the principles that determine the selection of the grammar on the basis of the restricted data available to the learner. It is a study of the nature of human intellectual capacities. In practice, the linguist studies both universal and particular grammar, aiming to characterize knowledge of a language and establish general properties of human intelligence.\n\n",
      "Deep and surface structures are related by formal operations, such as assigning the marker wh- to the most deeply embedded NP and replacing it with \"who\", deleting \"who is\", and inverting \"man\" and \"wise\". Grammatical transformations can generate infinite deep structures, which determine the semantic interpretation of a sentence. Ambiguity is resolved by extending the sentence, while deviant sentences can be explained by internalized rules. Deletion operations can also eliminate ambiguity, but require knowledge of the history of derivation of the structure.\n\n",
      "The surface structure of the sentence is misleading and uninformative, and the deep structure must represent underlying propositions. Each word can be represented as a set of specified features, and phonological rules are applied to produce the final phonetic representation.\n\n",
      "The underlying representation of right is /rixt/, with velar continuant /x/. The application of rules 23b and 23c gives [rayt] in isolation and [ray] in righteous. Rule 26 applies to the element /ti/ in /ignition/, /expeditious/, and /contrition/, replacing it with [], and rule 24 gives [ay] instead of [i] before -ious, -ion, etc., resulting in the double deviation from regular pattern seen in right-righteous.\n\n",
      "The sentences can be interpreted as either the man was seen by Mary as he was walking toward the railroad station, or that Mary saw the man and he was walking toward the railroad station. The A-over-A principle states that if a transformation applies to a structure of the form for any category A, then it must be so interpreted as to apply to the maximal phrase of the type A. Thus, the processes of interrogative and relative formation would be blocked in the first case, but not in the second.\n\n",
      "The A-over-A principle is proposed as a basis for an explanation of phenomena such as illustrated by examples 44-58. In each case, there is some reason to believe that the principle is appropriate, though there is evidence showing that it may be inadequately formulated or misconceived. Examples include conditions of deletion, cyclic application and transparency.\n\n",
      "Sentences of 60 are interpreted through an underlying structure of 61, which suggests a principle of erasure. This principle is violated in some cases, such as 63 and 68. A universal semantics may be proposed to explain the meaning of 70, but there is still much evidence to be accounted for. Structure-dependent operations, such as English interrogation, are used in human language instead of structure-independent operations, such as O1, O2, and O3.\n\n",
      "The text discusses the principles of universal grammar which provide a restrictive schema to which any human language must conform, as well as specific conditions determining how the grammar of any such language can be used. It suggests that knowledge of language is acquired on the basis of limited data and is largely independent of intelligence and experience.\n\n",
      "Universal phonology is more firmly established than universal syntax due to the complexity of the subject matter and lack of empirical control. Syntax is studied less deeply and with less success than phonetics. Several versions of the monograph \"Current Issues\" exist, none of which provide a satisfactory treatment. Universal grammar involves conditions on grammars, as well as principles to determine how they are interpreted.\n\n"
    ],
    "wordCount": 16916
  },
  {
    "tag": "h2",
    "header": "3 Linguistic contributions to the study of mind: future",
    "content": [
      "Human language is unique and distinct from animal communication. It is purposive, syntactic, and propositional in that it has an internal organization with structure and coherence that transmits information. Human language can be used to inform, mislead, clarify thoughts, or simply for play. It is not the same as animal communication which uses a finite number of signals each associated with a specific range of behavior or emotional state, or a finite number of linguistic dimensions each of which is associated with a nonlinguistic dimension. Understanding human language requires understanding the deeper underlying principles and abstract mental structures that it illuminates.\n\n",
      "Human language is based on entirely different principles than animal communication systems and appears to be associated with a specific type of mental organization. Generative grammars provide an explication of the Humboldtian idea of \"form of language\" and define a language as a recursively generated system where the laws of generation are fixed and invariant. Principles of universal grammar constrain the form and organization of any human language. The study of generative grammar points to a conceptual lacuna in psychological theory and may focus attention on the need for a concept of competence beyond the limits of behaviorist psychology. To construct a meaningful theory of learning, it is first necessary to characterize the underlying competence. Other areas of human competence may be studied using a similar approach.\n\n",
      "The underlying generative processes of language vary only slightly among languages and many interesting and unsolved questions remain as to how the choice of rules is determined by intrinsic, universal relations among features. There is an innate mental structure that is rich enough to account for the disparity between experience and knowledge, one that can account for the construction of empirically justified generative grammars within the given limitations of time and access to data. Furthermore, there is an upper and lower bound on the complexity that can be postulated as innate mental structure.\n\n",
      "The text discusses the debate between empiricist and rationalist approaches to understanding language acquisition, as well as critiques of the views of Nelson Goodman and Hilary Putnam. It argues that prelinguistic symbolic systems do not share certain properties with natural language, and that there are other ways to find evidence to support hypotheses about language structure. The argument concludes by saying that the properties proposed in universal grammar involve specific choices and conditions that cannot be explained by assumptions about memory capacity.\n\n",
      "The problem of language acquisition is one of determining the innate schema that characterizes the class of potential languages, followed by the study of the organism-environment interaction that sets the innate cognitive mechanism into operation, and finally, the task of determining what relation holds between a potential grammar and a set of data for it to be confirmed as the actual theory of a language.\n\n",
      "Hypothesis is confirmed – innate limitations on admissible hypotheses are a precondition for successful theory construction, and that the “guessing instinct” that provides hypotheses makes use of inductive procedures only for “corrective action.” Peirce maintained in this lecture that the history of early science shows that something approximating a correct theory was discovered with remarkable ease and rapidity. Closely related is the study of the biological bases of human language, an investigation to which Eric Lenneberg has made substantial contributions. In some cases at least, these built-in structures will degenerate unless appropriate stimulation takes place at an early stage in life. Comparative ethology was motivated by the hope that it would shed light on the a priori forms of human thought.\n\n",
      "Innate structure of the human mind is acquired through natural selection and is a product of natural laws. Natural selection determines possible successful mutation and complexity of organisms. Innate content of phonetic features determine functioning of phonological rules and universal formal conditions restrict choice of grammars.\n\n",
      "Modern attempts to train apes in behavior that investigators regard as language-like confirm incapacity. C. B. Ferster's report in Scientific American (May 1964) showed that hundreds of thousands of trials were required for 95% accuracy in a trivial task. Animal vocalization and communication (Thorpe, 1967) and the problem of serial order (Lashley, 1951) also show the limitation. The empirical assumptions of modern philosophy of language (Chomsky, 1969) and the Boas tradition (Joos, 1966) are discussed. Universal features of language are in flux (Chomsky & Halle, 1968). Innate schematism (Descartes, 1911) and general learning strategies (Goodman & Putnam, 1967) are considered. Cognitive capacities of young children (Mehler & Bever, 1967) and Kant's view of the apriori (Lorenz, 1941) are discussed.\n\n",
      "Mutations are rare and limited, so the range of possible modifications to chromosomes is also restricted. Evolution is thus shaped by these constraints.\n\n"
    ],
    "wordCount": 16070
  },
  {
    "tag": "h2",
    "header": "4 Form and meaning in natural languages",
    "content": [
      "Human language is unique among known systems of animal communication in its creative aspect, whereby a person can understand and produce expressions that are new to their experience and appropriate to the occasion. This creative activity is enabled by a system of rules that assigns sound and meaning in a definite way for an infinite class of possible sentences. Introspection can provide evidence about the sound-meaning relation, but cannot reveal the underlying rules and principles that determine it. The study of language seeks to uncover these properties of the human mind and understand how they are put to use. It has lead to the formulation of the theory of transformational-generative grammar, which proposes rules and principles governing the formation of language systems, and has resulted in productive research into refining and reconstructing these formulations. There is evidence that the ability to acquire and use language is a species-specific human capacity and is rooted in the specific character of the human mind.\n\n",
      "Grammatical structures and deep structures play a crucial role in determining meaning, with surface structure often being remote from deep structure. Grammatical processes, such as nominalization, can illustrate this phenomenon. The phonetic form of a sentence is determined by its surface structure, while deep structure is responsible for determining the meaning of a sentence. Surface structure can also have an impact on semantic interpretation, as with the present perfect aspect which carries the presupposition that the subject is alive. Intonation can also affect the presuppositions of a sentence.\n\n",
      "Deep and surface structure play roles in determining meaning: focus, presupposition, topic, comment, scope of logical elements, and pronominal reference are determined by surface structure; grammatical relations of predication, modification, etc. by deep structure. Placement of \"even\" changes presuppositions; placement of stress determines pronominal reference; reflexive pronouns refer differently depending on deep structure.\n\n",
      "The language-acquisition device must possess enough structure to construct a grammar from limited data, and the acquired grammars of different speakers must be similar. It is assumed that children are not genetically predisposed to learn one language over another. This leads to hypotheses about universal grammar and how it explains facts about other languages. Humans must have a rich set of mental attributes that allow them to learn and use a language from very little data, and they can use language creatively, constrained by its rules but free to express new thoughts. Control of human behavior by stimulus conditions, etc., cannot explain the range of human potentialities.\n\n"
    ],
    "wordCount": 6696
  },
  {
    "tag": "h2",
    "header": "5 The formal nature of language",
    "content": null,
    "wordCount": 0
  },
  {
    "tag": "h3",
    "header": "General Properties of language",
    "content": null,
    "wordCount": 82
  },
  {
    "tag": "h3",
    "header": "Competence and performance",
    "content": [
      "Command of a language involves understanding and producing signals with an intended semantic interpretation. Linguistic competence is the internalization of a system of rules that determine both the phonetic shape of a sentence and its intrinsic semantic content. Performance involves extralinguistic beliefs and cognitive structures like memory, which are not aspects of language. To study a language, one must attempt to separate various factors that affect performance. The goal of linguistic investigation is to discover the conditions any grammar must meet, including conditions on admissible phonetic and semantic representations and rules that generate paired phonetic and semantic representations. Human languages obey stringent limiting conditions and performance models must incorporate a grammar. Understanding a signal requires knowledge of a language's structure and production models must also incorporate a grammar. Competence is one factor among many that affects performance.\n\n"
    ],
    "wordCount": 952
  },
  {
    "tag": "h3",
    "header": "Initial steps toward a study of competence",
    "content": [
      "Human language is infinite in its set of paired phonetic and semantic representations generated by the grammar. Language contains devices for generating sentences of arbitrary complexity, so repetition is a rarity and innovation is the rule. There is no stock of verbal repertoire or patterns that a speaker uses; observation shows that language use relies on this unboundedness. To understand a language user's grammar we must observe their interpretation of sentences in terms of their semantic, grammatical and phonetic structure. This can be done by looking at the words they can insert in different sentence frames and the difference in meaning between those sentences. We can also hypothesize the existence of an acquisition model which selects a grammar based on the internal structure, methods of analysis and initial constraints imposed on any possible grammar.\n\n"
    ],
    "wordCount": 846
  },
  {
    "tag": "h3",
    "header": "Universal grammar",
    "content": [
      "Universal phonetics and semantics are proposed as language-independent systems of representation, with phonetic symbols analyzable into point and manner of articulation and semantic interpretations into fixed properties such as animate-inanimate, relational-absolute, agent-instrument, etc. It is assumed that the physical signal and semantic interpretation of a sentence are determined by universal principles from its representation. Bishop Wilkins' Essay Towards a Real Character and a Philosophical Language (1668) is an early example of this approach. Universal phonetics has been studied intensively, with considerable success; the parallel theory of universal semantics has been very little studied.\n\n"
    ],
    "wordCount": 440
  },
  {
    "tag": "h3",
    "header": "Universal grammar: universal phonetics",
    "content": [
      "Universal phonetics attempts to create a universal alphabet and system of laws that define the possible signals of any language. A signal is represented as a sequence of symbols of the phonetic alphabet. Two physical events might be regarded as repetitions in one language and nonrepetitions in another, so the universal alphabet must provide the means for distinguishing them. Symbols of the alphabet are sets of features such as voicing, frontness–backness, and stress, each with a specified value. The theory reflects language's discreteness (finite number of signals) and unboundedness (infinite signals). Primitive elements of the theory include distinctive features like labialization and velarization. Laws of universal phonetics attempt to govern permitted sequences and selection of features.\n\n"
    ],
    "wordCount": 717
  },
  {
    "tag": "h3",
    "header": "Universal grammar: universal semantics",
    "content": [
      "Semantic features and laws can be established to explain how concepts are determined and related. For example, \"knife\" is specified by physical properties and a \"good\" or \"terrible\" evaluation feature. This explains the semantic relationship between \"this is a good knife\" and \"this knife cuts well\". Universal semantics can become a substantive discipline if general principles are discovered regarding possible systems of concepts that can be represented in human language and the intrinsic connections among them.\n\n"
    ],
    "wordCount": 278
  },
  {
    "tag": "h3",
    "header": "Universal grammar: universal syntax",
    "content": [
      "Universal grammar is the study of the conditions that must be met by the grammars of all human languages. It consists of components for syntax, semantics, and phonology. Syntax is a set of abstract objects, deep and surface structures, which contain information relevant to semantic and phonetic interpretation respectively. Semantics assigns interpretations to deep structures, and phonology assigns phonetic interpretations to surface structures. The rules of syntax mediate the relationship between semantic and phonetic interpretations. Universal grammar studies the universal properties that define human language, such as the ability to make infinite use of finite means. Generative grammars are needed to determine regularities and general principles of language structure.\n\n"
    ],
    "wordCount": 1279
  },
  {
    "tag": "h3",
    "header": "Structure of the phonological component",
    "content": [
      "The phonological component of a generative grammar assigns a phonetic representation to a surface structure by using a set of linearly ordered rules applied in a cyclic fashion. These rules are specified in terms of the universal phonetic system, with columns corresponding to successive segments, rows corresponding to features, and each entry given as an integer value. To accommodate this, the surface structure is represented as a matrix with two values in each entry, and a proper bracketing with labeled brackets to indicate phrase types. In English, the distinctive features of the phonetic system have a classificatory role in the surface structure, and a phonetic role in the phonetic representation. To illustrate, consider the sentence \"John saw Bill,\" which is represented as a matrix and bracketed to indicate phrase types. Rules 7 and 8 then apply in the cyclic manner to assign stress, with rule 7 applying to nouns with two primary stresses, and rule 8 applying to all other units. For example, in \"John's black-board eraser,\" rule 7 assigns primary stress to \"black,\" weakening the stress on \"board\" to secondary, while rule 8 assigns primary stress to the right-most primary stressed vowel.\n\n",
      "The stress contour of English is a perceptual reality, determined by two rules (7 and 8) and the principle of the cycle. The hearer uses selected physical properties of the signal to identify the sentence and perceive its stress contour, even if it is not physically represented. Syntactic interpretation may be a prerequisite to hearing the phonetic representation in detail, and the evidence supports the view that the learner brings an innate schematism to language acquisition.\n\n"
    ],
    "wordCount": 3352
  },
  {
    "tag": "h3",
    "header": "Structure of the semantic component",
    "content": [
      "The semantic component of a generative grammar is the system of rules that converts a deep structure into a semantic representation that expresses the intrinsic meaning of the sentence. Deep structures are labeled bracketings of minimal \"meaning-bearing\" elements, and the interpretive rules apply cyclically to determine the semantic interpretation of a phrase from the interpretations of its immediate constituents and the grammatical relations represented. The subject–predicate relation holds between a noun phrase and a sentence of which it is an immediate constituent, and between a verb phrase and a sentence of which it is an immediate constituent. This relation can then be defined as the relation holding between the subject of a sentence and the predicate of this sentence. Universal grammar may provide language-independent characterizations of notions like \"noun phrase,\" \"verb phrase,\" and \"sentence,\" allowing for universal characterizations of grammatical relations. Whether the semantic component contains particular rules or principles of interpretation that belong to universal grammar remains an open question.\n\n"
    ],
    "wordCount": 1259
  },
  {
    "tag": "h3",
    "header": "Structure of the syntactic component",
    "content": [
      "The syntactic component of a grammar consists of a base system, which consists of a categorial system and a lexicon, and a transformational component. The base system generates deep structures which are labeled bracketings of sequences of formatives and junctures, while the transformational component maps these deep structures into associated surface structures. The categorial system is a context-free phrase-structure grammar, which defines grammatical relations using rules of the form A Z, where A is a category symbol and Z is a string of one or more symbols which may be category symbols or terminal symbols. The lexicon provides lexical entries containing phonological, semantic, and syntactic features, and redundancy rules which modify the feature content of lexical entries in terms of general regularities. The transformational component applies to base phrase-markers in a cyclic fashion to generate surface structures.\n\n",
      "Base phrase-markers generated by the categorial component and lexicon are deep structures that determine semantic interpretation. Transformational rules convert deep structures to surface structures. Rules of lexical insertion apply to phrase-markers, modifying them in specific ways. Transformations can form complex sentences where significant relations between parts are not directly represented.\n\n",
      "Deep structures are not adequately represented by surface structure, as indicated by ambiguous sentences such as 30a and 30b. The deep structure of 30a is 31 and the deep structure of 30b is 32, with different grammatical relations and semantic interpretations. Exchange of active and passive in the embedded sentence preserves meaning in 30b but not 30a. The underlying phrase-markers must be generated by base rules which can then be applied in sequence by transformational rules to produce a wide variety of surface structures.\n\n"
    ],
    "wordCount": 5406
  },
  {
    "tag": "h3",
    "header": "Concluding observations",
    "content": [
      "Human language must have a finite set of rules for producing an infinite number of sentences. Grammatical transformations are used to convert deep structures to surface structures, and also have a \"filtering effect\" by ruling out certain potential deep structures. These transformations help reduce the load on short-term memory, making speech perception more feasible. There is evidence that deep structures vary little between languages, and that categories of syntax, semantics, and phonetics are universal. It is likely that traditional universal grammar was not restrictive enough in its assumptions. A child's task is to determine which language he is being exposed to, using a predetermined class of grammars. He will then be able to know facts about the language without having experienced them. This suggests that humans have an innate schematism to interpret limited evidence, and that a theory of universal grammar is necessary to explain language acquisition.\n\n",
      "Innate mental structure is assumed to exist, and a theory of phonetic distinctive features is proposed. Semantic and syntactic structure are represented in deep structures and surface structures, respectively. Ambiguity is used to illustrate inadequacy of certain conceptions of syntactic structure. Language acquisition is based on evidence of deviant and anomalous utterances, and theories of universal phonetics have been refined and amplified.\n\n",
      "Examines the history and development of a particular field, with particular emphasis on recent trends and developments.\n\n"
    ],
    "wordCount": 4014
  },
  {
    "tag": "h2",
    "header": "6 Linguistics and philosophy",
    "content": [
      "Linguistics and philosophy share many similarities and ideas. Zeno Vendler has proposed that linguistics can be used to help with problems in analytic philosophy. A key concept in linguistics is the analysis of an utterance into a hierarchy of phrases, each belonging to a specific category. This is represented as labeled bracketing of the utterance. Transformational generative grammar posits that all surface structures are formed by application of such transformations from deep structures. Deep structures are more abstract and often reveal the semantic interpretation. There may be a system of universal semantics which specifies the class of possible semantic representations for a language like universal phonetics does for the class of possible phonetic representations. Referential opacity has revealed examples where replacement of one expression by another changes meaning. Linguistics may not be able to provide answers to philosophical questions, but it may provide insight into the nature of language.\n\n",
      "The text discusses the idea that certain facts related to sentences are important for the study of language, such as the fact that we can form nominal phrases corresponding to deep structures but not to surface structures. A research strategy is proposed for studying cognitive processes in humans, which includes developing a model of perception and a learning model. The theory of generative grammar is discussed and it is argued that if the theory is correct, it poses nontrivial problems for the theory of human learning.\n\n",
      "The problem of how this generative grammar is acquired can be described as a kind of theory construction, in which the child is presented with highly restricted data and constructs a theory of the language that this data represents. This theory has predictive scope beyond the data, and is constructed similarly by all normal language learners despite differences in experience and ability. An innate schematism is proposed to explain this uniformity, specificity, and richness of detail and structure in the grammars that are acquired, and second-language acquisition poses a similar problem, as much of the knowledge acquired is not the result of direct instruction.\n\n",
      "First-language acquisition is the problem of how humans acquire the ability to use language, and the empirical hypotheses regarding innate schematism have been proposed as a solution. Goodman argues that this problem is solved by the acquisition of a secondary symbolic system, but this is based on a metaphorical use of the term \"symbolic system\" and collapses when it is given a precise meaning. Goodman's argument is further flawed because the prelinguistic symbolic systems he proposes cannot be used for giving explanation and instruction in the same way as a first language. There have been attempts to formulate properties of universal grammar and their empirical consequences, which specify \"bad\" languages, but Goodman denies their existence due to ignorance. He also raises a vagueness issue regarding the distinction between Gruebleen-like and English-like languages, but this is not relevant to the problem of specifying the characteristics of universal grammar. Goodman's position is further undermined by his acceptance of the idea that some ideas are implanted in the mind as original equipment. Putnam's arguments are inconclusive due to certain erroneous assumptions about the nature of knowledge acquisition.\n\n",
      "Putnam argues that the only innate conditions that must be postulated are those that apply to all reasonable \"computing systems\" and no Behaviorist should feel any surprise at this. He claims phrase-structure rules are the \"simplest possible algorithm\" but this is untrue since there is no concept of \"ease of computation\" or \"simplicity of algorithm\" that even suggests this. He further proposes common origin of languages as a simpler hypothesis but this is irrelevant since the grammar must be discovered by the child on the basis of data available to him. Putnam's suggestion of using general multipurpose learning strategies is an empirical issue.\n\n",
      "Universal grammar is one element of a language acquisition theory, not the whole theory. It is used to select particular grammars from those that are compatible with empirical data. Simplicity of grammar is determined by a weighting function empirically determined by considering the relationship between input data and acquired grammars.\n\n",
      "Current date: 2023-02-02. It has been emphasized repeatedly that knowledge of a language is not a matter of \"knowing that\", but rather a matter of \"knowing how\". Harman's framework suggests that there are two kinds of knowledge: knowing that and knowing how. Universal grammar provides a schematism that can be used to acquire or use a language, and current work in linguistics supports a view of language and mind with distinct rationalist flavor. The concept of \"resourceful empiricism\" does not seem to be of much interest. These alternatives can be made fairly precise and investigated in terms of their empirical consequences.\n\n",
      "Language can clarify and substantiate conclusions about human knowledge related to philosophy of mind. Goodman and Putnam's contributions to Synthese, Vol. 17, No. 1, March 1967, pp. 12-28. Henry Hi and Gilbert Harman in Journal of Philosophy Vol. 64, No. 2, February 2, 1967, pp. 67-87. Language has no objective existence apart from mental representation. A. C. Fraser (ed.) in Locke's Essay Concerning Human Understanding, 1894 (reprinted by Dover, 1959). Distinction between Gruebleen and English. N. Goodman, Structure of Appearance, 2nd edn. (Indianapolis: Bobbs-Merrill, 1966). Generative capacity of transformational grammars, predicates definable over transformational derivations by intersection with regular languages. Finitary models of language users. Biological Foundations of Language (New York: Wiley, 1967). Weighting functions proposed in universal grammar. Matthews, Hidatsa Syntax (The Hague: Mouton, 1965). Peters and Ritchie, “On the Generative Capacity of Transformational Grammars”, Kimball, “Predicates Definable over Transformational Derivations by Intersection with Regular Languages”. Miller and Chomsky, “Finitary Models of Language Users”. Lenneberg, Biological Foundations of Language (New York: Wiley, 1967). Active-passive relation, sound-meaning correlation, generative grammar, concepts of segmentation and classification, phrase-structure grammar, study of co-occurrence relationships.\n\n"
    ],
    "wordCount": 15328
  },
  {
    "tag": "h2",
    "header": "7 Biolinguistics and the human capacity",
    "content": [
      "Language is a state of the faculty of language - an I-language in technical usage - that is the actual formulation of generative principles, not the set it enumerates. The biolinguistic approach views language as part of the natural world and focuses on a component of human biology related to language use and acquisition. This component is on par with the system of mammalian vision or insect navigation. The core questions are how much of language can be explained by principled explanations and how much is unique to this cognitive system. This has been called the minimalist program, but the questions apply to any biological system regardless of theoretical persuasion. The faculty of language is part of what Wallace called \"man's intellectual and moral nature\" which sets humans apart from other animals. It is commonly assumed that language is essential to this capacity and that it was the invention of language that triggered a rapid change in behavior, evidenced by the archaeological record. There is evidence that the precursors to language had a long evolutionary history, but the evolution of language itself may have been brief. Language is seen as virtually synonymous with symbolic thought and is believed to have come about secondarily as a communication system between individuals.\n\n",
      "Language evolved to enable abstract thought and planning, not to communicate directives for action. Its core semantics are based on complex internal elements rather than a relation between symbols and physical features of the external world. It has an unbounded capacity for combining symbols hierarchically to enable thought and interpretation. Modular learning and developmental constraints limit outcomes to optimal types. Evolutionary history likely contributes to nonoptimal solutions.\n\n",
      "Merge operation enables the formation of hierarchically structured expressions and is assumed to be a minimal requirement for a system of discrete infinity. Parameter setting in Principles and Parameters (P&P) framework facilitates the shift of burden of explanation from genetic endowment to language-independent principles and structural architecture, aiding in the investigation of evolution of language. Computational efficiency principles impose conditions on the optimal solution to link sound and meaning.\n\n",
      "The language faculty is complex and has many core properties that are specific to it. It requires formulable and enriching challenges to adequately describe and explain these properties. Achieving principled explanation is a daunting task, but one that could move us significantly beyond explanatory adequacy. This would require understanding how the states of the language faculty relate to general principles, possibly even those applicable to organisms generally. Furthermore, how mental properties relate to the organical structure of the brain is still unresolved and full of mystery for humans, as it is for insects.\n\n"
    ],
    "wordCount": 6166
  }
]
