[
  {
    "tag": "h2",
    "header": "Contents",
    "content": [
      "The book focuses on the way human beings make decisions. It introduces two systems of thinking, System 1 and System 2, and explores how they interact to shape our thoughts and judgments. System 1 is fast, automatic, and unconscious, while System 2 is slow, deliberate, and conscious. The author explains that System 1 is more prone to biases and heuristics, but it helps us to make quick decisions, especially in situations where time is limited. On the other hand, System 2 is more accurate and careful, but it requires effort, attention, and cognitive resources.\n\nThroughout the book, the author provides several examples to illustrate the limitations of human reasoning and judgment, such as the influence of cognitive ease, availability, and emotions on our decisions. He also shows how our thinking can be influenced by anchors, heuristics, biases, and illusions of understanding and validity. The author also discusses the role of expert intuition and the importance of using the \"outside view\" when making predictions and judgments.\n\nThe book also explores the concept of choice and decision-making, including the impact of the endowment effect, framing, and rare events on our decisions. The author also discusses the concept of the two selves and how our experiences and emotions shape our perception of well-being.\n\nIn conclusion, the book provides a comprehensive overview of the limitations and strengths of human reasoning and judgment, and offers practical insights into how we can improve our decision-making processes.\n\n"
    ],
    "wordCount": 197
  },
  {
    "tag": "h2",
    "header": "Introduction",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Part 1",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Two Systems",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "The Characters of the Story",
    "content": [
      "The book \"Thinking Fast and Slow\" by Daniel Kahneman discusses the two systems in our mind, System 1 and System 2. System 1 operates quickly and automatically with little effort or sense of voluntary control, while System 2 allocates attention to mental activities that demand it, including complex computations. It is associated with the subjective experience of agency, choice, and concentration. System 1 includes innate skills we share with other animals and mental activities that become automatic through prolonged practice, such as reading and understanding social situations. System 2 is responsible for conscious reasoning, decision making, and holding beliefs. System 2 can override the impulses and associations of System 1, but it requires effort and attention. The two systems interact continuously, with System 1 generating suggestions for System 2, which are then adopted or modified by System 2. The interaction of the two systems is a central theme of the book, and the author describes how we can be blind to the obvious and to our own blindness.\n\n",
      "The author describes two systems of thinking that occur in the mind, referred to as System 1 and System 2. System 1 is responsible for quick thinking and automatic responses, while System 2 takes over when things get difficult, requiring conscious attention and effort. System 1 has biases and can't be turned off, but it is still highly efficient in providing accurate models of familiar situations. Conflict arises between the two systems when there is a competition between an automatic reaction and an intention to control it. There are also illusions of thought, which are difficult to overcome because System 1 operates automatically and System 2 is too slow to take over. The author introduces these systems as useful fictions to help explain thoughts and actions of people, but acknowledges that they do not exist as real systems in the brain.\n\n",
      "The text discusses the idea of active agents who have distinct personalities, habits, and abilities. The author introduces the concept of System 1 and System 2 as nicknames for these agents, which are easier to remember and understand than more descriptive terms. The reason for this is that shorter terms occupy less working memory and reduce the ability to think. The text uses examples of System 1 and System 2 in the context of giving impressions and reacting to threats.\n\n"
    ],
    "wordCount": 4202
  },
  {
    "tag": "h2",
    "header": "Attention and Effort",
    "content": [
      "In this chapter, the author discusses mental effort and how it can be measured through changes in pupil size. The author describes an exercise called “Add-1” where people are asked to repeat strings of four digits and increment each digit by one, while maintaining a steady rhythm. This task puts a strain on cognitive abilities and causes the pupils to dilate as mental effort increases. The author also found that during routine conversation, the pupils remain small, indicating that it requires little or no effort, while mental activities like the \"Add-1\" task demand more effort and can cause the individual to become effectively blind. The author also notes that the allocation of attention has been honed over time through evolution, and in emergencies, System 1 takes over, assigning priority to self-protective actions. The author also mentions that he wrote a book called Attention and Effort based on his research with a graduate student, which focused on measuring mental effort through pupil dilation.\n\n",
      "The text explores the concept of System 2 and how it functions in the human mind. As skill and talent increase, less effort and energy is required, following the \"law of least effort\" which applies to both cognitive and physical exertion. The tasks that have been studied show that mental activities with higher demands for attention, such as memory recall and mathematical operations, result in larger pupil dilation. System 2 has the ability to follow rules, make deliberate choices, and adopt task sets, but is limited in its ability to deal with multiple distinct topics at once. Executive control and the ability to switch between tasks are also factors that contribute to effortful mental activity. Time pressure and the need to handle complex information can also lead to mental overload and increase effort. People usually avoid mental overload by breaking down tasks into smaller steps and relying on long-term memory or paper instead of working memory.\n\n"
    ],
    "wordCount": 3269
  },
  {
    "tag": "h2",
    "header": "The Lazy Controller",
    "content": [
      "The author of the book has a daily routine of walking four miles in the hills of Berkeley with a view of San Francisco Bay, and during this time he has learned about the effort required for physical and mental activities. He believes that System 2 has a natural speed, where monitoring what happens around you and inside your head requires little effort. However, when walking at an accelerated pace, it becomes difficult to think coherently and engage in mental work. The same applies to cognitive work, which requires self-control, attention, and effort. This is because both self-control and cognitive effort are forms of mental work, and they draw from a shared pool of mental energy. When people are challenged by a demanding cognitive task and a temptation, they tend to yield to the temptation. In addition, activities that require high demands on System 2 are depleting and unpleasant, resulting in a loss of motivation. On the other hand, activities that induce “flow”, a state of effortless concentration, can lead to an optimal experience. The nervous system consumes more glucose than other parts of the body, and effortful mental activity appears to be especially expensive in terms of glucose. The effect of ego depletion could be undone by ingesting glucose, as confirmed by several experiments. One such experiment involved volunteers who drank lemonade before participating in a task that required self-control. Those who drank lemonade sweetened with glucose showed no depletion effect, while those who drank lemonade sweetened with Splenda showed the expected depletion effect. A recent study showed that eight parole judges in Israel were affected by depletion effects in their judgment, which was reflected in the number of approved parole requests made throughout the day.\n\n",
      "The text describes the concept of two systems in the human mind. One is intuitive, impulsive and quick, while the other is capable of reasoning but can also be lazy. The authors explain this through examples such as a bat and ball puzzle, a syllogism, and a question about murder rates in Michigan. They also provide evidence from experiments that show a link between thinking and self-control, and between attention control and intelligence. A test was designed to measure the difference between individuals who are more like their System 1 or System 2 and it was found that those who are closer to System 1 are more impulsive, impatient and short-sighted, while those who are closer to System 2 are more cautious and rational.\n\n",
      "Keith Stanovich and Richard West introduced the terms System 1 and System 2. The authors have been researching the differences among individuals on biases of judgment. They found that there is a sharp distinction between two parts of System 2 and labeled them as separate \"minds.\" One deals with slow thinking and demanding computation and the other is rationality. High intelligence does not make people immune to biases because rationality should be distinguished from intelligence. Stanovich argues that superficial or “lazy” thinking is a flaw in the reflective mind, a failure of rationality. He believes that the bat-and-ball question and other similar questions are better indicators of susceptibility to cognitive errors than IQ tests. The distinction between intelligence and rationality could lead to new discoveries. The text mentions examples of individuals who are in a state of flow, who turn to standard operating procedures instead of thinking through problems, who don't bother checking if what they say makes sense or tend to say the first thing that comes to their mind, which might indicate weak System 2.\n\n"
    ],
    "wordCount": 4395
  },
  {
    "tag": "h2",
    "header": "The Associative Machine",
    "content": [
      "The chapter talks about how our mind processes information and the role of System 1 in this process. The author describes how our mind automatically links two words or ideas together to form a story or scenario, leading to an emotional response. This process is called associative activation and it works quickly, unconsciously, and effortlessly. Ideas are linked to other ideas through different types of connections such as cause and effect, things and their properties, or things and the categories they belong to. The mind activates many ideas at once, and only a few of these activated ideas make it to our conscious awareness.\n\nThe concept of priming is also discussed, which refers to the temporary increase in the ease of evoking related words or concepts after exposure to a specific word or idea. Priming effects can take many forms, such as faster recognition of related words or changes in behavior, thoughts, and emotions. The ideomotor effect is mentioned, which refers to the influence of an idea on an action. For example, holding a pencil in your mouth with the eraser end pointing up will unconsciously lead to a smile, while holding it with the point end pointing up will lead to a frown.\n\nThe author also mentions that a great deal of mental processing happens outside of our conscious awareness, and our thoughts and emotions can be unconsciously influenced by gestures and actions. The chapter concludes by emphasizing how we have limited knowledge of the workings of our minds and how much of our mental processing occurs without our conscious awareness.\n\n",
      "The text delves into the concept of priming, which refers to the idea that our thoughts and actions can be unconsciously influenced by subtle reminders in our environment. For instance, studies have shown that being exposed to money-related objects such as Monopoly money or a screen saver of dollar bills floating in water can prime people to become more independent and selfish. Another study showed that people who were primed by money chose to stay farther apart from others in a get-acquainted conversation. The idea of money primes individualism: a reluctance to be involved with others, to depend on them, or to accept demands from others. The author notes that the effects of priming are robust but not necessarily large, and that disbelief is not an option when it comes to accepting the major conclusions of these studies. They apply to everyone, including the reader. The author concludes with a demonstration of a priming effect where a banner poster displaying either flowers or eyes looking directly at the observer was displayed above the price list in an office kitchen. The average contributions to the honesty box changed significantly, with users contributing almost three times as much in “eye weeks” as they did in “flower weeks.” The effect occurred without any awareness. The author suggests that System 1, which provides impressions that often turn into beliefs, is often in control of what we do and provides a rapid, intuitive interpretation of the world around us, but also the origin of many systematic errors in our intuitions. The text ends with quotes from the author emphasizing the idea that the world makes much less sense than we think and that our mind is responsible for the coherence we perceive.\n\n"
    ],
    "wordCount": 3625
  },
  {
    "tag": "h2",
    "header": "Cognitive Ease",
    "content": [
      "The book delves into the concept of cognitive ease and how it affects our thinking and decision-making. Cognitive ease is a measure of how well things are going, whether there are any threats or major news, and whether extra effort is required by System 2. The level of cognitive ease is connected to a large network of inputs and outputs and can be influenced by things like font, mood, and the presence of unmet demands. When in a state of cognitive ease, people tend to be in a good mood, trust their intuition, and have casual and superficial thinking. Conversely, when in a state of cognitive strain, people are more vigilant, suspicious, and invest more effort into what they are doing. The book also covers illusions of remembering, truth, and writing persuasive messages. For example, the idea of familiarity gives the impression of truth, but frequent repetition can make people believe falsehoods. To write a persuasive message, the author suggests reducing cognitive strain by using high-quality paper, simple language, memorable ideas, and easy to pronounce names for sources. However, it's important to keep in mind that mental effort is aversive and that recipients want to avoid anything that reminds them of effort.\n\n",
      "The author discusses the idea that colors, rhyming or using simple language will not be effective if the message is contradictory to what the audience already knows. Psychologists believe that people are guided by their System 1 impressions, and often do not know the source of these impressions. A sense of cognitive ease is associated with feeling that a statement is true, but this feeling can also be caused by other factors such as font quality and attractive prose. Cognitive ease can be overcome when the motivation is strong, but otherwise the lazy System 2 will adopt the suggestions of System 1. Cognitive strain can mobilize System 2 and shift people's approach to a more engaged and analytic mode. The Cognitive Reflection Test found that performance improved with a bad font, as cognitive strain mobilizes System 2. The pleasure of cognitive ease is linked to good feelings, and the mere exposure effect occurs when repeated exposure to a stimulus is followed by nothing bad, resulting in a safety signal. Good mood, intuition, creativity, gullibility, and reliance on System 1 all go together, while sadness, vigilance, suspicion, an analytic approach, and increased effort also form a cluster. A good mood makes people more intuitive and creative but also less vigilant and prone to logical errors.\n\n",
      "The text describes the relationship between a good mood, cognitive ease, and intuition of coherence. The Remote Association Test shows that when people are presented with coherent word triads, they react with a slight smile and a feeling of cognitive ease. This impression of cognitive ease is also linked to positive affect. However, the relationship between cognitive ease and intuition of coherence is not necessarily causal. An experiment was conducted where participants were told about music played in their earphones that influenced their emotional reactions, which eliminated the intuition of coherence. This demonstrates that the brief emotional response to the presentation of the coherent triad of words is actually the basis of judgments of coherence. The automatic workings of System 1 have been extensively studied in recent decades and have revealed surprising findings about its influence on our perceptions and judgments. For example, bad font can influence truth judgments and improve cognitive performance, and familiarity can breed liking. Therefore, it's important to be aware of one's mood and cognitive state when making decisions or evaluating information.\n\n"
    ],
    "wordCount": 4501
  },
  {
    "tag": "h2",
    "header": "Norms, Surprises, and Causes",
    "content": [
      "In this chapter, the author explores how our mind understands our world and what we expect from it. Our brain has a model of the world that represents what is normal in it. This model is based on links between events, actions, and outcomes that occur regularly. When something unusual happens, it surprises us, and we classify it as either an actively expected event or a passively expected event. The latter means that it is normal in a situation but not probable enough to be actively expected. For example, meeting an acquaintance twice in different places in a short interval would seem almost normal even though it is unlikely. If a situation repeats itself, passive expectations can quickly turn into active expectations. Our brain can detect violations of normality very quickly and subtlety. We have norms for many categories, which provide the background for the immediate detection of anomalies.\n\nOur mind also tries to find causes and intentions behind events. For example, if someone is angry, we quickly understand why they are angry without having to think about it. System 1, our unconscious mind, automatically searches for causality and constructs a coherent story that links the fragments of knowledge it has. The automatic search for causes often leads us to make false conclusions, such as linking a major event to something else that happened at the same time. Our mind is adept at finding a coherent causal story that makes sense of the limited information it has.\n\n",
      "The event of a lost wallet can have various causes but when related to New York and crowds, people often associate it with a pickpocket being the cause. The concept of causality has been studied for centuries, and Albert Michotte published a book in 1945 that challenged traditional thinking about causality. The common belief was that we infer causality based on repeated observations of correlations among events, but Michotte argued that we see causality as directly as we see color. He conducted experiments showing that even infants as young as six months old perceive causality in events. At around the same time, Fritz Heider and Mary-Ann Simmel made a film demonstrating the perception of intentional causality. The film showed a large triangle bullying a smaller one and a circle, with the two joining forces to defeat the bully. This perception of intention and emotion is irresistible, except for people with autism. The psychologist Paul Bloom claims that our readiness to separate physical and intentional causality explains the universality of religious beliefs. Our mind is naturally ready to identify agents, assign personalities and intentions, and view actions as expressing individual propensities. The prominence of causal intuitions leads to people applying causal thinking inappropriately to situations that require statistical reasoning, but System 2 can learn to think statistically. The author sometimes refers to System 1 as an agent or as an associative machine, but they are just metaphors used to describe the mind's processes. People find it easier to think about the mind using traits and intentions, and this is why the author uses these metaphors. The author mentions that repetition is needed for a new experience to feel normal and that when surveying reactions to products, the entire range of normal reactions should be considered.\n\n"
    ],
    "wordCount": 3085
  },
  {
    "tag": "h2",
    "header": "A Machine for Jumping to Conclusions",
    "content": [
      "Danny Kaye once said that someone he dislikes is a person who jumps to conclusions. The author believes that this quote accurately describes how System 1 functions. Jumping to conclusions can be efficient under certain circumstances, such as when the conclusion is likely to be correct and the cost of an occasional mistake is acceptable, and if it saves time and effort. However, jumping to conclusions can also be risky when the situation is unfamiliar, the stakes are high, and there is no time to collect more information. \n\nSystem 1 can lead to intuitive errors because it often overlooks ambiguity and suppresses doubt. For example, the same shape can be read as a letter in a context of letters or as a number in a context of numbers, and the entire context helps determine the interpretation of each element. System 1 does not keep track of alternatives that it rejects and does not allow for conscious doubt, which is the domain of System 2. \n\nSystem 1 has a bias to believe and confirm, and its operations contribute to a general confirmation bias. This means that people are more likely to be influenced by persuasive messages when they are tired and depleted, and they tend to seek data that is compatible with their beliefs. \n\nThe halo effect is another example of how System 1 shapes our view of people and situations. If you like someone's politics, you are likely to like their voice and appearance as well. The halo effect increases the weight of first impressions and can sometimes result in subsequent information being ignored. \n\nThe author provides an example of how the halo effect influenced his grading of students' essays. He found that his evaluations were homogeneous and that the first essay had a disproportionate effect on the overall grade. To overcome this issue, he changed his grading method to avoid being biased by his first impression, but this resulted in a lower confidence in his grading.\n\n",
      "The text describes how System 1 and System 2 thinking play a role in our decision making process. System 1 is quick and intuitive, but has a tendency to jump to conclusions based on limited information. This tendency can result in biases such as overconfidence, framing effects, and base-rate neglect. The author suggests that to avoid these biases, it is important to seek out independent sources of information and guard against the influence of others. In meetings, asking all members to write a brief summary of their position before discussion can help to decorrelate errors and make use of the diversity of knowledge and opinions in the group. The author also introduces the concept of WYSIATI (What You See Is All There Is), which refers to the way the mind treats information that is currently available compared to information that is not.\n\n",
      "The text mentions that people often stick with the limited information they have and do not seek more that might contradict their current beliefs. This is referred to as WYSIATI - What You See Is All There Is.\n\n"
    ],
    "wordCount": 4017
  },
  {
    "tag": "h2",
    "header": "How Judgments Happen",
    "content": [
      "This chapter focuses on the two systems of our mind, System 1 and System 2, which play a crucial role in our judgments and decisions. System 1 operates continuously, monitoring what's happening both inside and outside of our mind and generating assessments of various aspects of a situation with little effort. These basic assessments are easily substituted for more difficult questions and this phenomenon forms the basis of the heuristics and biases approach. System 2 receives questions or generates them, directing attention and searching memory to find answers.\n\nOur ancestors' ability to assess the threat level of a situation has been passed down to us, and although less urgent in modern city environments, we still have this mechanism. Situations are evaluated as good or bad, requiring escape or allowing approach. The ability to discriminate between friend and foe at a glance is an example of this. The shape of one's face provides cues for assessing dominance, while facial expressions provide cues for assessing intentions. The accuracy of these assessments isn't perfect, but even an imperfect ability to assess strangers confers a survival advantage. This mechanism has been put to a new use in modern times, influencing how people vote. Research by Alex Todorov has shown that voters form an impression of how good a candidate will be in office based on their competence, which is judged by combining strength and trustworthiness.\n\nSystem 1 also understands language and performs computations of similarity and representativeness, attributions of causality, and evaluations of the availability of associations and exemplars even in the absence of a specific task set. However, it deals well with averages but poorly with sums, as the size of a category tends to be ignored in judgments of sum-like variables. In emotional contexts, the neglect of quantity is shown when participants showed little difference in their willingness to pay to save birds, despite the number of birds varying from group to group.\n\nFinally, System 1 has the aptitude of matching across diverse dimensions, as an underlying scale of intensity allows it to match different dimensions. For example, people can adjust the loudness of a sound to the severity of crimes or the punishment, thus feeling a sense of injustice if one tone is much louder than the other. Our ability to match Julie's reading prowess as a child to a height is another example of this aptitude.\n\n",
      "The text describes how people make predictions based on matching information from one scale to another. System 1 carries out many automatic computations and some of these are routine assessments, while others are voluntary only when the person intends them to be. The mental shotgun, or excess computation, occurs when System 1 does more than what System 2 charges it to do. This is because it's difficult to control System 1's computations. Two experiments are mentioned to demonstrate this concept. One experiment showed that participants were slower to recognize words as rhyming if they also compared their spelling, while the other experiment showed that people took longer to evaluate sentences as literally true if they could also be metaphorically true. The text also mentions that evaluating people as attractive is a basic assessment that happens automatically. The punishment for a crime must match its intensity for it to feel just. The text concludes by giving an example of a mental shotgun in action, where someone was asked about a company's financial soundness but couldn't ignore his positive feelings towards their product.\n\n"
    ],
    "wordCount": 2765
  },
  {
    "tag": "h2",
    "header": "Answering an Easier Question",
    "content": [
      "The mind is often not \"stumped\" and has intuitive feelings and opinions about everything. This is because if a difficult question can't be answered, the mind will find a related easier question to answer, which is the operation of substitution. The heuristics and biases approach was created when Kahneman and Amos were trying to understand how people make judgments of probability without knowing what it is. They concluded that people must simplify that impossible task and found that when called upon to judge probability, people actually judge something else and believe they have judged probability. System 1 often makes this move when faced with difficult questions if the answer to a related and easier question comes readily to mind. Substituting one question for another can be a good strategy, but in this case, the heuristics are not chosen and are a consequence of the imprecise control over targeting responses to questions. A heuristic alternative to careful reasoning is available and sometimes works fairly well, but sometimes leads to serious errors. A lazy System 2 often endorses a heuristic answer without much scrutiny, leading to an answer to a question that was not asked. The 3-D Heuristic is an example of substitution leading to a biased judgment if the objects appear more distant, they also appear larger on the page. The Mood Heuristic for Happiness shows how a question about dates would affect the answer to a question about happiness because the emotion from the dating question carried over to the happiness question. The Affect Heuristic goes further, showing that emotions often determine beliefs about the world, such as political preference.\n\n",
      "The text describes two systems in our brain, System 1 and System 2, which process information and impact our beliefs and attitudes. System 1 operates automatically and quickly, generating impressions, feelings, and inclinations, whereas System 2 is more analytical, deliberate, and in charge of self-criticism. However, when it comes to attitudes, System 2 tends to endorse the emotions of System 1 rather than challenge them. System 1 is characterized by a range of traits such as generating a coherent pattern of activated ideas, linking a sense of cognitive ease to illusions of truth, and being biased to believe and confirm existing beliefs. On the other hand, System 2 can substitute an easier question for a difficult one, be more sensitive to changes than states, and respond more strongly to losses than gains. This distinction between the two systems has implications for decision making, as System 1 influences our thinking and behavior.\n\n"
    ],
    "wordCount": 2752
  },
  {
    "tag": "h2",
    "header": "Part 2",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Heuristics and Biases",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "The Law of Small Numbers",
    "content": [
      "The chapter discusses the relationship between our minds and statistics. The author starts with an example of a study of the incidence of kidney cancer in the United States counties, which shows a pattern of low incidence in rural, sparsely populated, and Republican states in the Midwest, South, and West. However, this pattern is not due to the rural lifestyle or the political affiliation but because of the small populations in these counties. The mind's intuition for causation often leads us to make false conclusions about such patterns, which are actually just artifacts of sampling. The author then reflects on his own experience with statistics as a research psychologist and the belief in the law of small numbers, which leads to poor intuitions about sample size selection. He and Amos conducted a study to test the statistical intuitions of experts and found that they too made similar mistakes, leading to the recommendation that researchers replace their impression formation with computation. The chapter also touches on the bias of confidence over doubt, where people tend to focus more on the gist of the story than the details.\n\n",
      "People tend to give more importance to the story and not to the source of the information. The system 1 of the mind does not doubt and tends to suppress ambiguity and creates a coherent story as possible. System 2 is capable of doubt but is harder work than certainty. People tend to believe in small sample size and consistency, which leads to a halo effect and a view of reality that is simpler and more coherent than the data supports. People are prone to see causality instead of statistical regularities, which can lead to misperceptions of randomness. A basketball study showed that there is no such thing as a hot hand, but people still believe in it. People are far too willing to reject the belief that much of what we see in life is random. An example of this is the Gates Foundation's investment in creating small schools based on a survey of successful schools, but the same survey shows that bad schools also tend to be small, leading to the conclusion that small schools are not better on average but simply more variable.\n\n",
      "The text highlights that drawing causal explanations from observations made through statistics can often be misleading. It is important to take into account the possibility of chance events causing certain facts and patterns observed in the world. The text also mentions the law of small numbers, which states that drawing conclusions from a small sample of observations can lead to inaccurate inferences. It is advised to wait for a larger sample before reaching any conclusions, as this helps to avoid pressure to reach a premature decision.\n\n"
    ],
    "wordCount": 4146
  },
  {
    "tag": "h2",
    "header": "Anchors",
    "content": [
      "\"Thinking Fast and Slow\" by Daniel Kahneman explores the two systems of thinking we have: System 1 and System 2. In this chapter, \"Anchors\", Kahneman explains how the effects of anchoring can be seen in our everyday lives. Anchoring is when people consider a particular value for an unknown quantity before estimating that quantity. This leads to estimates that stay close to that number even if it is absolutely not informative. For instance, in an experiment conducted by Kahneman and his colleague, students were asked to write down a number after spinning a wheel of fortune marked from 0 to 100, which stopped either at 10 or 65. Participants were then asked whether the percentage of African nations among UN members was larger or smaller than the number they wrote down and what their best guess of the percentage of African nations in the UN was. The average estimates of those who saw 10 and 65 were 25% and 45%, respectively. There are two mechanisms that produce anchoring effects: one is deliberate adjustment, an operation of System 2, and the other is priming effect, an automatic manifestation of System 1. The adjust-and-anchor heuristic is a strategy for estimating uncertain quantities where people start from an anchor, assess whether it is too high or too low, and gradually adjust their estimate by mentally “moving” from the anchor. However, this adjustment typically ends prematurely, leading to insufficient adjustment. On the other hand, anchoring as priming effect is when System 1 tries to construct a world in which the anchor is the true number, leading to selective activation of compatible memories. Anchoring can be measured and is an impressively large effect. The difference between the effects of high and low anchors can be calculated as the anchoring index and values of 55% are typical.\n\n",
      "The real-estate agents were given an opportunity to assess the value of a house that was actually on the market. The agents were asked about the factors that influenced their judgment, but they insisted that the asking price had no effect on their responses. However, the anchoring effect was found to be 41%, with the professionals almost as susceptible to anchoring effects as business school students with no real-estate experience. Anchoring effects are also found in decisions about money and environmental causes. The difference between the high-anchor and low-anchor groups was $123, indicating that increasing the initial request by $100 brought a return of $30 in average willingness to pay. Anchors have been shown to have a powerful effect even when they are random and do not offer any information. Negotiators who focus their attention and search their memory for arguments against the anchor can reduce the anchoring effect. A cap on the size of damages in personal injury cases would benefit serious offenders and large firms much more than small ones, as the anchor would pull up the size of many awards. The power of random anchors is due to the associative activation of System 1 and the susceptibility of System 2 to biasing influence. The content of a screen saver or a story, even if it is a lie, will have the same effect on the associative memory regardless of its reliability. Priming and anchoring both threaten the sense of agency and autonomy by influencing thoughts and behavior without one's awareness or control.\n\n",
      "The text mentions the idea of anchoring, which refers to a phenomenon where people are influenced by an initial reference point, such as a number, in making subsequent decisions. This can be used in negotiations, where one party tries to get the other anchored on a certain number, and it can also impact the outcome of court cases if a lawyer introduces a low number for damages, which can influence the judge's decision.\n\n"
    ],
    "wordCount": 4053
  },
  {
    "tag": "h2",
    "header": "The Science of Availability",
    "content": [
      "The availability heuristic is a mental shortcut that affects our judgments and decisions by considering the ease with which examples come to mind. This heuristic is often used when we want to estimate the frequency of a category or the likelihood of an event. However, this process can lead to systematic errors because it substitutes one question for another - instead of estimating the size of a category or the frequency of an event, we report an impression of the ease with which examples come to mind. There are many potential sources of bias in the availability heuristic, such as personal experiences, vivid examples, and attention-grabbing events. To resist these biases, one must make an effort to reconsider their impressions and intuitions and ask questions like, \"Could it be that I feel no need to get a flu shot because none of my acquaintances got the flu last year?\" Maintaining vigilance against these biases is important, especially in situations where credit allocation is at stake, such as in marriages or collaborative projects. \n\nStudies have shown that people's impressions of the frequency of a category can be affected by the requirement to list a specified number of examples. For instance, if someone is asked to list six instances of assertive behavior, they will rate themselves as more assertive than if they were asked to list twelve instances. The reason for this is that the first few examples come easily to mind, but retrieval soon becomes much harder, leading to the inference that if it is difficult to come up with examples, then the person cannot be very assertive. However, these judgments can be disrupted if the experience of fluency is given a spurious explanation, such as background music affecting performance in the memory task. In conclusion, the availability heuristic can have a significant impact on our judgments and decisions, but by being aware of its potential biases and making an effort to resist them, we can improve the accuracy of our assessments.\n\n",
      "In the text, the authors discuss a phenomenon in reasoning where subjects have an experience of decreasing fluency as they try to come up with instances. It is expected that difficulty in coming up with new instances will increase, but it actually increases more rapidly than expected. This low fluency influences people's judgments. However, when the surprise is eliminated, low fluency no longer affects the judgment. The authors suggest that this process is carried out by System 1 and 2, which are capable of setting expectations and being surprised when those expectations are violated. The authors also discuss how personal involvement in a task can influence the way people judge things. People who are more engaged with the task tend to focus on the content rather than the ease of retrieval. The authors also mention several conditions that make people more susceptible to availability biases, including being in a good mood, being knowledgeable novices, and having faith in intuition. The authors also suggest that reliance on intuition is not just a personality trait, but can be influenced by reminding people of a time when they had power. The text also mentions several examples of availability biases, including overestimating or underestimating risks based on recent events or media coverage.\n\n"
    ],
    "wordCount": 2818
  },
  {
    "tag": "h2",
    "header": "Availability, Emotion, and Risk",
    "content": [
      "Risk and emotions are important concepts in the field of psychology. People often make decisions based on how easily they can recall information about certain topics, which is referred to as availability. This idea was first observed by economists and researchers studying the patterns of insurance purchases after natural disasters. They discovered that people tend to be more diligent in purchasing insurance after a disaster but their memories and concern fade over time. Researchers went on to study public perception of risks, and found that the ease with which people can recall information about risks and the emotional reactions they have to these risks are linked. For example, frightening thoughts and images are easier to recall and exacerbate fear. This idea was further developed into a concept known as the affect heuristic, where people make decisions based on their emotions, like whether they like or dislike something. The same experiment also showed that changing the emotional appeal of a technology changes people's beliefs about its benefits and risks. On the other hand, experts tend to be better at dealing with numbers and probabilities, but their judgments and preferences about risks may still differ from those of the general public. One scholar argues that the definition of risk is subjective and depends on the choice of measure, and that policies should consider the different views of both experts and citizens. Another scholar believes that objective analysis of costs and benefits, guided by science and expertise, will lead to better regulation. However, this view also acknowledges that biases can flow into policy and contribute to erratic priorities. A process known as the availability cascade, in which media coverage of a risk can trigger public panic and large-scale government action, is also discussed.\n\n",
      "The text describes the concept of \"availability cascades\" in which media coverage and public concern creates a cycle of fear. This process is sometimes accelerated by \"availability entrepreneurs\" who work to keep the flow of worrying news constant. The media's competition for attention-grabbing headlines often exaggerates the danger, making it difficult for scientists or others to dampen the increasing fear. The issue becomes politically important because it is on everyone's mind, leading the political system to respond based on the intensity of public sentiment. The authors focus on two controversial examples, Love Canal and Alar, to illustrate the concept of availability cascades. Love Canal involved buried toxic waste that contaminated the water, while Alar was a chemical sprayed on apples that was thought to cause cancer. Both events received intense media coverage and caused fear, but they also led to greater awareness and resources being allocated to reduce the risk. The text also discusses the limitations of the mind's ability to deal with small risks and the impact of irrational fears on public policy. The author believes that the combination of probability neglect and availability cascades leads to an exaggeration of minor threats. They conclude that psychology should inform the design of risk policies that balance the expertise of decision makers with the public's emotions and intuitions.\n\n"
    ],
    "wordCount": 3241
  },
  {
    "tag": "h2",
    "header": "Tom W’s Specialty",
    "content": [
      "The chapter focuses on the concept of representativeness, which refers to the tendency for people to judge probability based on how similar a person or event is to a stereotype. This heuristic can lead to errors in judgment, as it often ignores important base-rate information, such as the overall frequency of an event. The author uses the example of a personality sketch of a graduate student named Tom W, who was described by a psychologist as intelligent but lacking creativity and with a need for order. Participants were asked to rank the likelihood that Tom W was enrolled in different fields of specialization, including business administration, computer science, and social sciences. When asked to rank the fields based on their probability of enrollment, participants ranked the small specialties like computer science highly, as they fit the description of Tom W. However, when considering the base-rates of the different fields, it was more likely that Tom W was enrolled in a larger field, like humanities and education. This example demonstrates that people tend to rely on representativeness in making predictions, even when the information provided is unreliable, and the results can be biased. The author suggests that while the representativeness heuristic can have some validity, exclusive reliance on it can lead to statistical errors and ignoring base-rate information.\n\n",
      "A woman who is described as \"a shy poetry lover\" is more likely to study business administration than Chinese literature, according to the base rate frequency of enrollment in each field. People without training in statistics use base rates in predictions under some conditions. However, concern for base rates disappears as soon as a personality is described. Instructions to \"think like a statistician\" enhance the use of base rates, while instructions to \"think like a clinician\" have the opposite effect. A recent experiment with Harvard undergraduates found that frowning, an expression which increases the vigilance of System 2 and reduces overconfidence, led to sensitivity to base rates. The second sin of representativeness is insensitivity to the quality of evidence, and WYSIATI makes it difficult to reject worthless information. The correct answer to a problem like the Tom W puzzle is to stay close to prior beliefs and let the base rates dominate estimates. Bayesian reasoning is influenced by base rates and the diagnosticity of evidence, but intuitive impressions of diagnosticity are often exaggerated. The keys to disciplined Bayesian reasoning are to anchor on a plausible base rate and question the diagnosticity of evidence.\n\n"
    ],
    "wordCount": 3439
  },
  {
    "tag": "h2",
    "header": "Linda: Less Is More",
    "content": [
      "The Linda problem is a famous experiment in the field of psychology that was created by Daniel Kahneman and Amos Tversky to demonstrate the role of heuristics in judgment and their incompatibility with logic. Participants were presented with a list of eight possible scenarios for a fictional lady named Linda, including being a teacher, bookstore worker, feminist, psychiatric social worker, bank teller, and more. The critical items in the list were whether Linda looked more like a bank teller or a feminist bank teller. People agreed that Linda fit the description of a feminist bank teller better than a bank teller, even though the probability of her being a feminist bank teller must be lower than the probability of her being a bank teller. The results showed that 89% of undergraduate students and 85% of doctoral students in decision science violated the logic of probability. The reasoning behind this error was due to the idea of representativeness, where people judged the probability of events by how similar they were to stereotypes or prototypes. This is known as the conjunction fallacy, where people judge a conjunction of two events to be more probable than one of the events in a direct comparison. Another example was given, where people were asked to price dinnerware sets with different numbers of pieces, and those who were shown both sets were willing to pay more for the larger set, while those who were only shown one set valued the smaller set more. This is known as the less is more phenomenon, where removing items from a set can actually increase its value.\n\n",
      "In this text, the author discusses the concept of \"less is more,\" where adding a positive item to a set can actually decrease its value. This phenomenon is seen in studies involving dinnerware sets and baseball cards, as well as in the famous \"Linda problem.\" The author also explores the violation of logic in probability judgments, such as the ranking of Wimbledon outcomes and the preference for betting on less likely sequences in a die-rolling experiment. Another study found that framing a question as \"how many\" instead of \"what percentage\" can reduce the incidence of conjunction fallacies. The author reflects on the reception of the Linda problem in the scientific community, noting that the focus on this case led to criticism of their approach to judgment and a small dent in credibility. However, they consider it a fact of life that scientific debates often focus on weaknesses rather than strengths, and accept the attention brought to their work by the controversy.\n\n"
    ],
    "wordCount": 3725
  },
  {
    "tag": "h2",
    "header": "Causes Trump Statistics",
    "content": [
      "In the book Thinking Fast and Slow, the author explores how our minds process information and make decisions. One of the key points is that we often rely on base rates, which are facts about a certain population, to make quick judgments about individuals within that population. However, our reliance on base rates varies depending on whether the base rate is seen as a statistical fact or a causal one. Statistical base rates are often neglected or underweighted when specific information about an individual is available, while causal base rates can easily be combined with other case-specific information to form stereotypes. The author notes that while resistance to stereotypes is a desirable moral position, it can result in suboptimal judgments.\n\nIn an experiment, participants were more likely to judge an individual student as likely to pass an exam if they were told the class had a high success rate rather than a low one. This shows that people are highly sensitive to causal base rates, as the difficulty of the exam was seen as a factor affecting individual outcomes. However, another study showed that people will not draw from base-rate information an inference that conflicts with their beliefs. This supports the conclusion that teaching psychology is largely ineffective, as people will cling to their beliefs even in the face of evidence to the contrary.\n\nThe author concludes by saying that while resistance to stereotyping is important in creating a more equal society, it is important to acknowledge that neglecting valid stereotypes can result in inaccurate judgments. He notes that the costs of stereotyping are worth paying to achieve a better society, but denying that these costs exist is not scientifically defensible.\n\n",
      "The experiment described in the text was aimed at showing that even normal, decent people might not rush to help during a seizure if they expect others to take on the unpleasantness of dealing with it. The psychology professor wants the students to learn that a surprisingly high rate of failure implies a very difficult test because a potent feature of the situation, such as diffusion of responsibility, induces normal and decent people to behave in a surprisingly unhelpful way. The students are expected to change their view of human nature for the worse about themselves but Nisbett and Borgida suspect that students would resist the work and the unpleasantness. To find out if their beliefs about human nature really changed, Nisbett and Borgida showed them videos of brief interviews allegedly conducted with two people who had participated in the experiment. The students were asked to guess how quickly the interviewee had come to the aid of the stranger, but their predictions remained the same even after being exposed to the statistical results of the experiment. Nisbett and Borgida found that surprising individual cases have a powerful impact and are a more effective tool for teaching psychology because the incongruity must be resolved and embedded in a causal story. The test of learning psychology is whether the individual's understanding of situations has changed, not whether they have learned a new fact. Compelling causal statistics will not change long-held beliefs or beliefs rooted in personal experience, but surprising individual cases can have a powerful impact.\n\n"
    ],
    "wordCount": 3345
  },
  {
    "tag": "h2",
    "header": "Regression to the Mean",
    "content": [
      "In this chapter, the author recounts a story of teaching flight instructors in the Israeli Air Force about the principle of rewards for improved performance being better than punishment for mistakes. After he finished his speech, one of the instructors shared that he had found the opposite to be true in his experience as a flight instructor. The author then explains that what the instructor experienced is known as regression to the mean, which is the phenomenon where a random event will tend to return to its average value over time. The instructor had praised the flight cadets for their good performance, but that performance was due to chance and therefore likely to decline in the future. Similarly, the instructor punished the cadets for poor performance, but that too was due to chance and therefore likely to improve in the future. The author then goes on to explain the concept of regression to the mean with the example of a high-level golf tournament. He explains that the more extreme a player's score is on day 1, the more regression to the mean they can expect on day 2, because their luck on day 1 was probably a factor in their success. He also explains that regression to the mean occurs when predicting an early event from a later event, and that it is a mathematically inevitable consequence of luck playing a role in the outcome. The phenomenon of regression to the mean is strange to the human mind, and was first identified and understood in the 19th century by Sir Francis Galton.\n\n",
      "Regression to the mean is a phenomenon where two variables that are not perfectly correlated will have a regression effect. This occurs when one variable influences the other, causing it to return to the average over time. This concept was first discovered by Francis Galton, who worked for several years to understand the relationship between correlation and regression. He was helped by the most brilliant statisticians of his time. The correlation coefficient, which varies between 0 and 1, measures the relative weight of the factors shared by two variables. The higher the coefficient, the more the variables are related. If the correlation between two variables is less than perfect, there will be regression to the mean. This idea is often difficult for people to understand because our minds are biased towards seeking causal explanations, even when there are none. This can lead to incorrect interpretations of regression effects, which can cause problems in research. For example, if you wanted to determine if an energy drink was effective in treating depression, you would need to compare a group of patients who received the treatment to a control group that received no treatment or a placebo. If the control group improved simply due to regression to the mean, then the treated group would need to improve more to conclude that the treatment was effective.\n\n"
    ],
    "wordCount": 3741
  },
  {
    "tag": "h2",
    "header": "Taming Intuitive Predictions",
    "content": [
      "The human brain is capable of making predictions in various aspects of life, from financial earnings to the time required to complete a project, from the demand for a dish in a restaurant to the number of trucks needed to put out a fire. These predictions can be made either by looking at data and making calculations, or through intuition and System 1. Some intuitive predictions are based on a person's skill and expertise acquired through repeated experience, while others are based on heuristics that may lead to non-regressive assessments of weak evidence.\n\nAn example of an intuitive prediction is provided with Julie, a senior in a state university who read fluently when she was four years old. People quickly associate her early reading with a high GPA, even though there is only a weak correlation between the two. This is because System 1 automatically associates the evidence with the target of the prediction and evaluates the evidence in relation to a relevant norm. The final step involves translating the evaluation into a prediction. \n\nHowever, this process of intuitive prediction is biased and ignores regression to the mean. The correct way to make a prediction is to start with an estimate of average GPA, then determine the GPA that matches the impression of the evidence, estimate the correlation between the evidence and GPA, and finally, move a percentage of the distance from the average to the matching GPA that is equal to the estimated correlation.\n\nThis approach can be applied to predict any quantitative variable, such as profit from an investment or the growth of a company, and helps to produce a more moderate prediction that takes into account both intuition and actual data.\n\n",
      "The passage discusses the concept of intuition and how it can be moderated and corrected. This is because intuition is not regressive and therefore biased. Predictions based on intuition need to be corrected, as they are often overly optimistic or pessimistic. This can be done by finding the relevant reference category, estimating the baseline prediction, and evaluating the quality of evidence. However, some people may prefer extreme predictions even though they are biased, as they provide comfort. The corrective procedures proposed can help people think about how much they know and make them choose between their intuition and the evidence. The author also mentions that regression to the mean is difficult to understand and requires special training. Ultimately, the author suggests that predictions should be moderate and unbiased, allowing for a range of uncertainty, but also acknowledges that some people may need the security of distorted estimates to avoid paralysis.\n\n"
    ],
    "wordCount": 3921
  },
  {
    "tag": "h2",
    "header": "Part 3",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Overconfidence",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "The Illusion of Understanding",
    "content": [
      "The author of this book, Daniel Kahneman, is discussing the idea of a narrative fallacy. This fallacy refers to the way we humans tend to create stories about the past and present that are simple, concrete and assign more importance to skills and intentions while ignoring the role of luck. The problem with these stories is that they mislead us into thinking that we understand the world and its events when in fact we don't. For example, if we look at the story of how Google became a giant in the technology industry, it appears that two creative students came up with a better way of searching information on the internet and made a series of successful decisions that led to their company becoming one of the most valuable stocks in America. However, the story doesn't mention the countless events that could have gone wrong or the hapless competitors who were blind and slow in dealing with the threat. In reality, luck played a much bigger role in Google's success than we realize.\n\nWhen people change their minds about something, they often have difficulty remembering their previous beliefs. This is called the \"I-knew-it-all-along\" effect, or hindsight bias. It leads us to evaluate decisions based on their outcomes rather than on the process that was followed. This makes it difficult to properly evaluate decision makers, such as physicians, financial advisors, third-base coaches, CEOs, social workers, diplomats and politicians. Observers are prone to blame decision makers for bad outcomes even though the decision was sound at the time it was made. This outcome bias is especially unkind to decision makers who act as agents for others.\n\n",
      "The Central Intelligence Agency obtained information that al-Qaeda might be planning a major attack against the United States but they brought it to National Security Adviser Condoleezza Rice, instead of President George W. Bush on July 10, 2001. The hindsight bias is greater when the consequence is worse and in the case of a catastrophe such as 9/11, people are more ready to believe that the officials who failed to anticipate it were negligent or blind. Adherence to standard operating procedures is driven by decision makers who expect to have their decisions scrutinized with hindsight, leading to bureaucratic solutions and reluctance to take risks. Increased accountability also brings undeserved rewards to irresponsible risk seekers and lucky gamblers. The sense-making machinery of System 1 makes us see the world as more tidy, simple, predictable, and coherent than it really is. Business books often provide illusory certainty, satisfying the need for a clear message about the determinants of success and failure in business. However, research shows that leaders and management practices do influence the outcomes of firms in the market to a lesser extent than suggested by the business press. A correlation coefficient between success of the firm and the quality of its CEO is estimated to be 0.30, indicating 30% overlap. This means that the stronger CEO will lead the more successful firm 60% of the time, only a 10% improvement over random guessing. Many business books tend to exaggerate the impact of leadership style and management practices on firm outcomes, leading to illusions of understanding. The halo effect and outcome bias combine to explain the appeal of these books. Stories of success and failure in business offer a simple message of triumph and failure that identifies clear causes and ignores the determinative power of luck and regression to the mean.\n\n"
    ],
    "wordCount": 3789
  },
  {
    "tag": "h2",
    "header": "The Illusion of Validity",
    "content": [
      "The author tells a story about his experience as an evaluator of soldiers in the Israeli army. He and his colleague were assigned to evaluate candidates for officer training, using methods that had been developed by the British army during World War II. The soldiers took part in the \"leaderless group challenge,\" where they had to work together to cross an obstacle field with a long log without touching the ground or the wall. The author observed their behavior under stress and made note of who took charge, who was cooperative, who was hot-tempered, etc. Based on these observations, he and his colleague made predictions about which soldiers would be eligible for officer training. However, every few months they received feedback about the cadets' performance at the officer-training school, and their predictions were found to be mostly useless. Despite this knowledge, the author and his colleague continued to feel confident in their specific predictions. This is an example of the illusion of validity, where subjective confidence in a judgment is not a reasoned evaluation of the probability that the judgment is correct but rather reflects the coherence of the information and the cognitive ease of processing it.\n\nThe author also discusses the illusion of stock-picking skill in the investment industry. Many people believe they know more about what the price of a stock should be than the market does, but this belief is an illusion. The standard theory of how the stock market works is that a stock's price reflects all the available knowledge about its value and the best predictions about its future. However, many individual investors lose consistently by trading, as demonstrated by finance professor Terry Odean's study of 10,000 brokerage accounts over a seven-year period. The conclusion is that the theory of perfect prices protecting fools from their own folly is not quite right, and many individual investors lose money by trading.\n\n",
      "An investor made two stock transactions, one to buy and the other to sell, revealing their expectations for the future performance of these stocks. However, research by Odean and his colleague showed that individual traders' predictions were wrong on average, as the shares they sold performed better than those they bought. The most active traders had the poorest results and men acted on their ideas more often than women, resulting in women achieving better investment results. Professional investors were more selective in responding to news compared to individual investors, who tend to lock in their gains by selling \"winners\" and hang on to their losers. Research showed that the majority of mutual funds underperformed the overall market and that stock pickers were playing a game of chance. An examination of 25 anonymous wealth advisers' investment outcomes showed that their success was mostly due to luck and not skill. The illusion of skill is deeply ingrained in the culture of the investment industry, and even when faced with statistical evidence challenging these beliefs, individuals tend to ignore them and rely on their personal experience. There is also a tendency to believe that the future is predictable based on past events and accounts offered by financial pundits, leading to overconfidence in forecasting abilities.\n\n",
      "The topic being discussed is the idea that large historical events are determined by luck, and the concept of expert predictions in politics and economics. Philip Tetlock, a psychologist, conducted a study that involved 284 people who made their living commenting or offering advice on political and economic trends. He asked them to make predictions about future events and gathered over 80,000 predictions. The results showed that the experts performed worse than if they had simply assigned equal probabilities to each outcome, even in the region they were most knowledgeable of. The study also found that the more famous the forecaster, the more overconfident they were, and they often resisted admitting error. Tetlock categorizes those who make predictions into two groups: hedgehogs, who have a theory about the world and are confident in their forecasts, and foxes, who recognize that reality emerges from the interactions of many factors and are less likely to be invited to participate in television debates. The main point is that errors of prediction are inevitable due to the unpredictability of the world, and high subjective confidence is not a reliable indicator of accuracy. The line separating the predictable future from the unpredictable distant future has not been drawn yet. The author mentions the concept of illusions of validity and skill and stresses that the question is not whether the experts are well trained, but whether their world is predictable.\n\n"
    ],
    "wordCount": 5326
  },
  {
    "tag": "h2",
    "header": "Intuitions vs. Formulas",
    "content": [
      "The chapter focuses on the idea that formulas are often better than human intuition in making predictions. The author cites the work of psychologist Paul Meehl, who conducted a review of 20 studies that compared clinical predictions made by trained professionals to statistical predictions made by combining scores and ratings according to a rule. The results showed that the formula was more accurate in 11 out of 14 cases. Further research has expanded the range of predicted outcomes to include variables such as the longevity of cancer patients, the length of hospital stays, and the diagnosis of cardiac disease, and has consistently shown that simple algorithms are often more accurate than expert judgment. One reason for this is that humans are inconsistent in their judgments and are subject to context dependency, while formulas always return the same answer given the same input. Another reason is that experts tend to be overconfident in their intuitions and assign too much weight to personal impressions, while formulas are not affected by such biases. The chapter also mentions the work of economist Orley Ashenfelter, who used a simple formula to predict the future value of Bordeaux wines, and showed that it was more accurate than the opinions of world-renowned experts. The author concludes that, to maximize predictive accuracy, decisions should be left to formulas, especially in low-validity environments where predictability is poor.\n\n",
      "The Apgar test was developed by Dr. Virginia Apgar as a way to assess the health of newborn babies one minute after they were born. The test involves rating five variables - heart rate, respiration, reflex, muscle tone, and color - with scores ranging from 0 to 2, depending on the robustness of each sign. A baby with a total score of 8 or above is considered to be in good shape, while a baby with a score of 4 or below is in need of immediate intervention. Checklists and simple rules, like the Apgar test, have been credited for reducing infant mortality. Clinical psychologists initially rejected the idea of statistical predictions in favor of intuitive judgments, but the line between what clinicians can do well and what they cannot do well is not clear. In 1955, the author was assigned to set up an interview system for the Israeli Defense Forces and decided to use a statistical formula instead of relying on global evaluations. The new interview procedure was a substantial improvement over the previous method, although far from perfect. The results of the study showed that the sum of the six ratings was a better predictor of soldiers' performance than the previous interviewing method, and the intuitive judgment of the interviewers also did well.\n\n",
      "The author learned a valuable lesson about the role of intuition in selection interviews while working on a project for the army. The author created a formula that gave equal weight to an intuitive evaluation and objective information gathered through separate trait ratings. Years later, the author visited their old army base and discovered that the interviewing practices were still in use and had not changed much. The author suggests that this method can be applied to other tasks such as hiring sales representatives. The process involves selecting a few key traits, developing questions to assess each trait, and scoring each trait before moving on to the next one. It is important to avoid halo effects and to make a final decision based on the candidate's overall score. The author believes that using this method will lead to better hiring outcomes compared to relying on intuitive judgment alone. The author suggests that whenever human judgment can be replaced by a formula, it should at least be considered.\n\n"
    ],
    "wordCount": 4633
  },
  {
    "tag": "h2",
    "header": "Expert Intuition: When Can We Trust It?",
    "content": [
      "In this chapter, the author discusses the concept of expert intuition and when it can be trusted. The author starts by explaining how professional controversies in academic circles often result in exchanges of criticism and rejoinders that are unproductive and can be a waste of time. To deal with disagreements, the author has engaged in adversarial collaborations with other scholars who hold opposing views. In this case, the author partnered with Gary Klein, the leader of an association of scholars and practitioners who reject the focus on biases in decision making and prefer to study real-life experts. The author and Gary Klein set out to answer the question of when you can trust an experienced professional's intuition. Over the course of several years, they had many discussions, resolved many disagreements, and eventually published a joint article. The author also mentions Malcolm Gladwell's book Blink, which praises intuition but is criticized by the author for not properly investigating why the art experts in the story had a gut feeling that the statue was a fake.\n\nThe author then goes on to describe the contrasting experiences that shaped their views on intuition. The author's thinking was shaped by observing the illusion of validity in oneself and reading Paul Meehl's demonstrations of the inferiority of clinical prediction. In contrast, Gary Klein's views were shaped by his early studies of fireground commanders who made good decisions without comparing options. This led to the recognition-primed decision model, which involves both System 1 (automatic) and System 2 (deliberate) processes. Intuition is described as nothing more than recognition, a result of information stored in memory. Emotional learning, such as learned fears and hopes, can be quickly acquired, but expertise in complex tasks like high-level chess or firefighting takes years of dedicated practice to develop. The acquisition of expertise involves becoming familiar with thousands of configurations and being able to recognize recurrent patterns.\n\n",
      "The text describes the concept of intuition and expertise, and when to trust self-confident professionals who claim to have an intuition. The two authors agree that the confidence people have in their intuitions is not a reliable guide to their validity. The environment, feedback, and practice are key factors in acquiring a skill, but some environments are more challenging than others, and some aspects of any professional's tasks are easier to learn than others. The quality and speed of feedback, as well as enough practice opportunities, are essential in developing intuitive skills. However, experts may be overconfident due to the unrecognized limits of their expertise. To evaluate an expert's intuition, you need to consider whether the environment is sufficiently regular, whether there was adequate opportunity to learn the cues, and whether the judgment is based on valid information. Subjective confidence is not a good diagnostic of accuracy, as substitution may occur automatically, and judgments that answer the wrong question can also be made with high confidence.\n\n",
      "The idea of evaluating an expert's intuition involves looking at the regularity of the environment and the expert's learning history, rather than relying solely on their confidence. The authors initially realized that certain professions, such as fireground commanders and pediatric nurses, would have more valid intuitions compared to others, like stock pickers and pundits, as studied by Meehl. Despite disagreements and differences in attitude, emotions, and taste, the authors were able to reach intellectual agreement on most of the substantive issues raised during the project. When evaluating an expert's intuition, factors that should be considered include the expertise in a specific task, amount of practice, belief in the regularity of the environment, subjective confidence, and opportunities for learning with clear feedback.\n\n"
    ],
    "wordCount": 4394
  },
  {
    "tag": "h2",
    "header": "The Outside View",
    "content": [
      "The author and a group of people were tasked with creating a curriculum to teach high school students judgment and decision making. They had been working on it for about a year and thought they were making good progress. One day, the author asked everyone to write down an estimate of how long it would take them to submit a finished draft of the textbook to the Ministry of Education. The estimates were centered around two years. Then the author asked their curriculum expert, Seymour, whether he could think of other similar teams that had developed a curriculum from scratch. Seymour was familiar with several such teams and estimated that about 40% of these teams failed to complete their task. Those who finished took no less than seven years and no more than ten. When the author asked Seymour to compare their skills and resources to those of the other teams, Seymour answered that they were below average, but not by much. This was a complete surprise and caused great concern among the team, who had never considered the possibility of failure. Despite this, the team continued working on the project, which ended up taking eight years to complete. The author learned three lessons from this experience: the distinction between the inside view and the outside view, the planning fallacy and irrational perseverance. The inside view is when people focus on their specific circumstances and search for evidence in their own experiences. The outside view is when people direct their attention away from themselves and towards a class of similar cases. The author's team displayed the planning fallacy by failing to allow for unknown unknowns and basing their forecast on the information in front of them. They also displayed irrational perseverance by failing to abandon the project despite knowing that it had a 40% chance of failure. The team ignored the outside view and preferred the inside view, which is common and sometimes carries moral overtones. The author compared this experience to an experiment that suggested the futility of teaching psychology, where students completely neglected global results they had just learned when making predictions about individual cases.\n\n",
      "A case is unique and every medical professional has a pride about it. However, there is still some ambivalence about the outside view in the medical profession that expresses concerns about the impersonality of procedures that are guided by statistics and checklists. The Planning Fallacy refers to overly optimistic forecasts of the outcome of projects and is found everywhere. Examples of the planning fallacy abound in the experiences of individuals, governments, and businesses and can lead to overestimating benefits and underestimating costs. One solution to the planning fallacy is the outside view, which involves using statistical information from similar ventures to make a baseline prediction and adjust it based on specific information about the case. A well-run organization will penalize planners for failing to anticipate difficulties and reward them for precise execution. People often take on risky projects because they are overly optimistic about the odds they face. The main point of the curriculum story is the author's realization that he failed as chief and inept leader who should have ensured that major problems were properly discussed by the team. The authors of unrealistic plans are often driven by the desire to get the plan approved. The sunk-cost fallacy is the tendency to continue a decision because of the investment of resources, even when the decision was not rational to begin with. Taking an outside view is not the natural thing to do and requires effort.\n\n"
    ],
    "wordCount": 3827
  },
  {
    "tag": "h2",
    "header": "The Engine of Capitalism",
    "content": [
      "The author discusses the concept of optimism and its impact on decision making. Optimism is a common trait among individuals who view the world in a more positive light, have a greater sense of well-being, and are more resilient when faced with challenges. This optimistic bias can be both a blessing and a risk, and it is particularly significant when it comes to decision making. Optimistic individuals are often the inventors, entrepreneurs, and leaders who shape our lives, taking on risks and seeking challenges. However, their overconfidence can lead to costly mistakes, such as excessive risk-taking, unrealistic expectations, and a disregard for relevant information. Entrepreneurs, for example, tend to overestimate their chances of success, even when the odds are stacked against them. And leaders of large businesses sometimes make unsound bets, driven by the belief that they are better equipped to manage assets than current owners. This overconfidence is compounded when these individuals receive recognition and praise, leading to an increase in compensation and a focus on activities outside of their companies. The author notes that while entrepreneurial optimism contributes to the economic dynamism of a capitalistic society, there are difficult policy issues to consider when it comes to supporting small businesses. Additionally, the author highlights the role of cognitive biases, such as the planning fallacy, in contributing to entrepreneurial optimism. People often focus on their goals, plans, and skills while neglecting the plans and skills of others, leading to an illusion of control and overconfidence in their beliefs.\n\n",
      "People tend to overestimate their abilities and rate themselves as above average, particularly in tasks they perform moderately well. This overoptimism may extend to entrepreneurs who believe their start-up’s success is largely dependent on their own efforts. However, this neglects the impact of competition and changes in the market. A similar phenomenon is evident in financial forecasting, where executives are often overconfident in their predictions and can lead organizations to take unnecessary risks. The tendency towards overconfidence is driven by System 1 processes and social pressures, which favor appearing knowledgeable and confident over acknowledging uncertainty. Despite attempts to mitigate overconfidence through training, it remains a persistent problem. A possible solution is a premortem, a group session where individuals imagine a future disaster and write about its history. This helps overcome groupthink and encourages consideration of alternative scenarios.\n\n",
      "When a decision is made in a team, it can trigger the imagination of knowledgeable individuals to move in a specific direction. However, as the team converges on the decision and the leader shows her stance, doubts about the decision are suppressed and seen as a lack of loyalty to the team and its leaders. This suppression of doubt leads to overconfidence in the group, but it helps to reduce the impact of biases such as WYSIATI and uncritical optimism. The individuals in the group may have an illusion of control and seriously underestimate obstacles, neglecting competitors and believing they know more than they actually do. To help prevent nasty surprises, it's suggested to conduct a premortem session where someone may raise a previously neglected threat.\n\n"
    ],
    "wordCount": 4176
  },
  {
    "tag": "h2",
    "header": "Part 4",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Choices",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Bernoulli’s Errors",
    "content": [
      "In the 1970s, a Swiss economist named Bruno Frey wrote an essay about the psychological assumptions of economic theory. The essay stated that the rational agents of economic theory were rational, selfish, and had stable tastes. However, psychologists knew that people were neither fully rational nor completely selfish, and their tastes were anything but stable. The psychologists knew that people had a System 1, which limited their view of the world based on the information available at the moment and made them inconsistent and illogical. The psychologists, Amos and Daniel, decided to focus on decision making and study people's attitudes towards risky options. They sought to answer a specific question: What rules govern people's choices between different simple gambles and between gambles and sure things? Simple gambles provide a model that shares important features with more complex decisions, and they represent the fact that the consequences of choices are never certain. The field of decision making had a theory, expected utility theory, which was the foundation of the rational-agent model. Expected utility theory was a logic of choice based on the axioms of rationality, but it was not intended as a psychological model. Amos and Daniel, as psychologists, set out to understand how humans actually make risky choices without assuming anything about their rationality. They conducted experiments by inventing simple decision problems and asking themselves how they would choose. Five years later, they completed an essay titled \"Prospect Theory: An Analysis of Decision under Risk.\" Their theory was a modification of expected utility theory, and it explained systematic violations of the axioms of rationality in choices between gambles. The theory was published in Econometrica and turned out to be the most significant work they ever did. Two years later, they published an account of framing effects, which showed the large changes of preferences caused by inconsequential variations in the wording of a choice problem. Bernoulli’s Error was a term coined by Bernoulli himself, a mathematician who looked for a function that related subjective experience (utility) to actual amounts of money. Bernoulli argued that a gift of 10 ducats had the same utility to someone who already had 100 ducats as a gift of 20 ducats to someone whose current wealth was 200 ducats. Bernoulli introduced a new approach to the evaluation of gambles, where people's choices were based on the psychological values of outcomes, their utilities, rather than their dollar values. The psychological value of a gamble was the average of the utilities of its possible outcomes, each weighted by its probability. Bernoulli proposed that the diminishing marginal value of wealth was what explained risk aversion, the common preference for a sure thing over a favorable gamble of equal or slightly higher expected value.\n\n",
      "Bernoulli's theory of expected utility, also known as \"moral expectation\", was a groundbreaking concept in economics that explains why people make decisions based on risk. Bernoulli applied this theory to calculate how much a merchant would be willing to pay for insurance for a shipment of spices. The theory assumes that people's happiness is determined by their wealth and therefore, people with the same amount of wealth should have the same level of happiness. However, this theory fails to take into account reference points, which play a crucial role in determining a person's happiness. For example, two people with the same wealth may have different levels of happiness depending on their recent change in wealth relative to their reference points. Similarly, two people with different starting wealth may make different choices when offered a gamble or a sure thing. Current wealth matters a great deal and people think of gains or losses rather than states of wealth. Despite its flaws, Bernoulli's theory has stood the test of time and is still used in economic analysis today.\n\n"
    ],
    "wordCount": 3389
  },
  {
    "tag": "h2",
    "header": "Prospect Theory",
    "content": [
      "Daniel Kahneman and Amos Tversky, the authors of Thinking Fast and Slow, stumbled upon a crucial flaw in the traditional economic theory of decision-making by happenstance. They were studying how people evaluate the value of money by asking them to make choices about small gambles involving pennies. However, they realized that this was not a good way to study the subject since it failed to take into account the direct value of money. They then remembered a theory by Harry Markowitz, which stated that the utility of money is attached to changes in wealth rather than to the state of wealth. This led Kahneman and Tversky to focus on outcomes as gains and losses, rather than states of wealth. This shift in perspective allowed them to explore the idea that people are risk-averse when faced with positive options and risk-seeking when faced with negative options. This is due to the fact that losing money is more painful than gaining an equivalent amount of money is pleasurable. This observation led Kahneman and Tversky to develop prospect theory, which is a more complex theory of decision-making than the traditional utility theory. Prospect theory takes into account three essential principles: evaluation is relative to a neutral reference point, there is diminishing sensitivity to changes in wealth, and loss aversion. Loss aversion refers to the fact that people are more sensitive to losses than to gains. The authors also explain that this asymmetry between the power of positive and negative experiences has an evolutionary history and is common to many automatic processes of perception, judgment, and emotion.\n\n",
      "The text discusses the concept of loss aversion, where losses are considered to have a greater impact on decision-making than gains. The fear of losing is more intense than the hope of gaining. This is demonstrated through a coin toss gamble where most people reject the gamble due to the fear of losing $100 which is more intense than the hope of gaining $150. There is a range for the loss aversion ratio and it varies from one person to another, but the average is 1.5 to 2.5. Professional risk takers in finance tend to be less loss averse as they do not respond emotionally to every fluctuation. However, the loss aversion coefficient increases when the stakes are higher and may even be infinite if the possible loss is ruinous or threatens one's lifestyle. The text also points out the flaws of prospect theory, such as the assumption that the reference point has a value of zero leading to absurd consequences and failure to acknowledge disappointment and regret in decision-making. Despite its flaws, prospect theory was accepted because it added new concepts, such as the reference point and loss aversion, which provided new predictions that turned out to be true.\n\n",
      "The text mentions a person who turns down favorable opportunities due to his emotional response to trivial gains and losses. Despite having vast wealth, the person's behavior doesn't make sense. It is noted that this person weighs losses about twice as much as gains, which is considered normal.\n\n"
    ],
    "wordCount": 4036
  },
  {
    "tag": "h2",
    "header": "The Endowment Effect",
    "content": [
      "In the book \"Thinking Fast and Slow\" by Daniel Kahneman, the author writes about the endowment effect. This is a phenomenon in which owning an item increases its perceived value. For example, if you have a ticket to a sold-out concert, you might not be willing to sell it for more than a certain price even though you would have been willing to pay more than that amount to buy the ticket. This is because the act of giving up something you own is associated with a sense of loss.\n\nThe concept of the endowment effect is based on the idea of loss aversion, which means that people are more sensitive to losses than to gains. According to Kahneman, this is because the response to a loss is stronger than the response to a corresponding gain. The endowment effect is not universal and does not always occur. For example, if someone asks you to change a $5 bill for five singles, you might not feel any sense of loss.\n\nThe theory behind the endowment effect was developed by Richard Thaler, who was a graduate student in economics at the University of Rochester in the 1970s. Thaler collected observations of behavior that could not be explained by standard economic theories and came across the endowment effect while observing a professor's reluctance to sell a bottle of wine. Thaler realized that the professor's willingness to buy or sell the bottle depended on whether or not he owned it at the time. If he owned it, the act of giving it up would be associated with a sense of loss.\n\nThaler worked closely with Amos Tversky and Jack Knetsch to study the endowment effect and their findings have been a significant milestone in the development of behavioral economics. They found that the endowment effect is not universal and is dependent on the particular item in question.\n\nIn conclusion, the endowment effect is a phenomenon in which owning an item increases its perceived value. It is based on the idea of loss aversion and is not universal, with the degree of loss aversion varying depending on the item. The study of the endowment effect has been important in the development of behavioral economics and has helped to shed light on why people make certain choices.\n\n",
      "The text describes an experiment designed to understand the contrast between goods that are held for exchange and for use. A variation of the method created by Vernon Smith, the founder of experimental economics, was used where a limited number of tokens were distributed to participants in a market. The tokens were exchanged for cash and had value only because they could be traded. After conducting trades for tokens, the experimenters conducted a similar market for an object held for use, a coffee mug. Unlike the token market, the average selling price was about double the average buying price, and the estimated number of trades was less than half of what was predicted by standard theory. This experiment showed that people are reluctant to give up an object they expect to use, a phenomenon called the endowment effect. Evidence from brain imaging confirmed that selling goods one would normally use activates regions of the brain associated with disgust and pain. However, the endowment effect can be eliminated when reference points change, such as with experienced traders or people living below a reference point, making the asymmetry between the pleasure of getting and the pain of giving up irrelevant.\n\n",
      "The text discusses the difference between how poor people and traders think about money. The poor are not indifferent to gaining or losing, as they see all their choices as a loss. They view money spent as a loss of another good that could have been purchased. On the other hand, some well-off people may find spending painful. There may also be cultural differences in attitudes towards spending money on minor luxuries. Experiments conducted on students in the US show larger buying and selling price disparities compared to English students. The endowment effect refers to the reluctance to part with something once it has been acquired. This can result in difficulties in making concessions during negotiations, as losses loom larger than gains. When prices are raised, demand decreases and people may hate the idea of selling something for less money than they paid for it, which is an example of loss aversion. The text also mentions that some people treat any dollar they spend as a loss.\n\n"
    ],
    "wordCount": 4272
  },
  {
    "tag": "h2",
    "header": "Bad Events",
    "content": [
      "The author explains the concept of loss aversion in behavioral economics, and how it is the most significant contribution of psychology to this field. He then goes on to explain that losses have a larger impact on people than gains. This is due to the negativity dominance, which is wired into the biology of humans and animals. Negativity attracts attention faster, and bad events leave a stronger impact than good ones. The amygdala, the threat center of the brain, quickly responds to even symbolic threats, such as emotionally loaded words or opinions with which one strongly disagrees. The reference point for evaluating outcomes as losses or gains can be the status quo or a future goal. People tend to work harder to avoid a loss rather than achieve a gain. The example of golfers putting more accurately for par than birdie was used to illustrate this concept. The same can be seen in negotiations where people are more willing to compromise when they are negotiating over an expanding pie, but when the pie is shrinking and losses need to be allocated, negotiations become more difficult.\n\n",
      "The text outlines the concept of loss aversion and its impact on human behavior, both in personal life and in institutions. The principle explains why people tend to resist change and cling to their current situation when faced with the possibility of losing something. The idea is that losses loom larger than gains, making the reference point or \"entitlement\" more important for people to preserve. This force of conservatism helps maintain stability in our relationships and jobs, but also makes it difficult to implement reforms. The concept of loss aversion has also been applied to legal decisions and the administration of justice, where people who lose are seen as deserving more protection from the law. The text also mentions research on fairness in economic transactions, which found that the public draws a distinction between actual losses and foregone gains. People are more likely to punish a firm that exploits its power to break informal contracts and impose losses on others, rather than reward a firm that shares its gains. The influence of loss aversion extends beyond financial transactions, impacting the way people view changes in rental prices, for example.\n\n"
    ],
    "wordCount": 3803
  },
  {
    "tag": "h2",
    "header": "The Fourfold Pattern",
    "content": [
      "The author starts the chapter by discussing how people weigh different characteristics when forming an overall opinion about a complex object, such as a car, a person, or an uncertain situation. They explain that this weighting occurs unconsciously and is often influenced by System 1 thinking. The expectation principle, which states that the more probable an outcome, the more weight it should have in terms of evaluating the psychological value of the outcome, is often not followed in practice. People often overweight unlikely outcomes and underweight almost certain outcomes, leading to inconsistencies with the expectation principle. This leads to the Allais paradox, where people's preferences violate the rules of rational choice. The author and their colleague carried out a study to measure decision weights and found that decision weights depart sharply from probabilities near the extremes, with unlikely events being considerably overweighted and almost certain events being underweighted.\n\n",
      "The text mentions a concept of \"The Fourfold Pattern\" of preferences in decision making. According to the author, people attach values to gains and losses rather than to wealth, and the decision weights they assign to outcomes are different from probabilities. The fourfold pattern consists of four cells, each showing a different scenario of making decisions with either positive or negative prospects. The top left cell shows people being risk averse when there is a substantial chance of a large gain. The bottom left cell explains why lotteries are popular. The bottom right cell is where insurance is bought, where people are willing to pay much more for insurance than expected value. The top right cell is where people take desperate gambles to avoid a large loss, which often leads to disasters. The legal scholar Chris Guthrie has applied the fourfold pattern to two situations in which the plaintiff and the defendant in a civil suit consider a possible settlement. The difference in the strength of the plaintiff's case determines the plaintiff's risk aversion and the defendant's risk seeking behavior in negotiations. The fourfold pattern was confirmed by experiments and analyses of actual negotiations.\n\n",
      "The text speaks about the fourfold pattern in decision making and how it can lead to deviations from expected value. Both parties involved in a lawsuit are aware of the probability of success, but the plaintiff may be aggressive in negotiations due to the small chance of winning a large amount. On the other hand, the defendant may opt for a settlement to avoid the small risk of a bad outcome. Overweighting of small probabilities is common in intuitive decision making and can cost an organization in the long run. For example, settling all 200 frivolous lawsuits each year for $100,000 instead of going to trial will result in a total loss of $20 million. Both risk-aversion and risk-seeking behavior can lead to inferior outcomes if they deviate from expected value. It is important to consider the long-term effects of decisions and not let emotions interfere with the evaluation of probable outcomes.\n\n"
    ],
    "wordCount": 4523
  },
  {
    "tag": "h2",
    "header": "Rare Events",
    "content": [
      "The chapter discusses how people respond to rare events, such as terrorist attacks or winning the lottery. The author shares his own experience of avoiding buses in Israel during a period of frequent suicide bombings. He explains that this behavior was driven by an \"availability cascade,\" where the emotional arousal caused by vivid images of death and damage became highly accessible and produced an impulse for protective action. The author argues that system 1 (emotional and instinctive) cannot be turned off and that the emotion is disproportionate and insensitive to the actual level of probability. The chapter also explores how people make judgments about the likelihood of rare events and the impact of these events on their decisions. The author argues that people often overestimate the likelihood of rare events, and this is due to the influence of emotions, vividness, and cognitive ease on fluency, availability, and probability judgments. He provides examples of how people's judgments can be influenced by the specificity of the event and alternative outcomes. The author also discusses the impact of affect-laden imagery on the response to probability and argues that while emotional outcomes can overwhelm the response to probability, money gambles are the exception as they have a definite expected value.\n\n",
      "An experiment was conducted to determine the effect of a vague estimate on the likelihood of finding an attractive cash gift. Participants were asked to assess the cash equivalent of gambles with a 21% chance to win. It was found that the difference between the high-probability and low-probability gambles was much more pronounced for money than for roses. To rule out the influence of emotions, the researchers compared the willingness to pay to avoid gambles, but results showed that the intensity of emotion was not the answer. Another experiment revealed that adding explicit price information did not alter the results, and people did not use price information as an anchor in evaluating the gamble. The hypothesis is that a rich and vivid representation of the outcome, whether or not it is emotional, reduces the role of probability in the evaluation of an uncertain prospect. This idea is supported by other observations, such as denominator neglect, in which people tend to overweight low-probability events when described in terms of relative frequencies. The format in which risks are expressed can affect the evaluation of risks and create opportunities for manipulation. The hypothesis is that focal attention and salience contribute to the overestimation of unlikely events and the overweighting of unlikely outcomes. Salience is enhanced by mere mention of an event, vividness, and format in which probability is described. In choice from description, rare outcomes are overweighted relative to their probability, but this overweighting is never observed in choice from experience and underweighting is common.\n\n",
      "According to the text, there is evidence that people tend to underweight rare events when making decisions, such as choosing a restaurant or preparing for earthquakes. One reason for this is that many people never experience the event. However, even when people have experienced the rare event, they still tend to underweight it. This occurs because the mind generates global representations of things and assigns emotional attitudes and tendencies to approach or avoid them. Rare events may be overweighted if they specifically attract attention, but otherwise they are likely to be overlooked. The probability of a rare event will often be overestimated due to a confirmatory bias in memory, and obsessive concerns, vivid images, concrete representations, and explicit reminders can all contribute to overweighting. On the other hand, neglect may occur if none of these factors are present. In general, the mind is not designed to accurately assess the probability of rare events.\n\n"
    ],
    "wordCount": 4707
  },
  {
    "tag": "h2",
    "header": "Risk Policies",
    "content": [
      "The chapter talks about the concept of risk and how people make decisions regarding risks. In one experiment, two choices were presented, where in one choice a person could either pick a sure gain of $240 or 25% chance to gain $1,000 and 75% chance to gain nothing, and in the second choice, a person could either choose a sure loss of $750 or 75% chance to lose $1,000 and 25% chance to lose nothing. The majority of people chose the sure gain and avoided the sure loss, which shows that people tend to be risk averse when it comes to gains and risk-seeking when it comes to losses. This tendency is due to the emotional reaction of System 1, which occurs before the more effortful computation of expected values.\n\nThe chapter also talks about the difference between narrow framing and broad framing and how narrow framing can lead to suboptimal decisions. For example, if a person is faced with 5 simple binary decisions, narrow framing would result in a sequence of 5 choices, while broad framing would result in a single choice with 32 options. Broad framing is superior as it takes into account all the decisions and their interconnections, but humans tend to be narrow framers due to their susceptibility to WYSIATI (What You See Is All There Is) and aversion to mental effort.\n\nThe chapter also talks about Samuelson's problem, which is a classic example that demonstrates the limitations of utility theory. A friend of Samuelson was asked whether he would accept a gamble in which he could lose $100 or win $200. The friend refused to take the gamble because he felt that the loss of $100 was more intense than the gain of $200. However, if he was offered 100 such gambles, he would take it up. This contradicts the concept of rationality and highlights the absurdity of severe loss aversion for small bets. The chapter concludes by saying that the aggregation of favorable gambles can reduce the probability of losing and the impact of loss aversion, and people can benefit financially if they follow this advice.\n\n",
      "The text discusses the emotional response of subjects to gains and losses, which is measured by changes in skin conductance. The results show that broad framing reduces emotional reactions to losses and increases willingness to take risks. On the other hand, the combination of loss aversion and narrow framing is a costly curse for individual investors. To avoid this, they can reduce the frequency of checking their investments, which will save them time and emotional stress. Checking daily fluctuations causes more pain from small losses than pleasure from small gains. Checking once a quarter is enough, and may even be too much. By avoiding short-term outcomes, individuals can improve both the quality of their decisions and the outcomes themselves. They are also less likely to engage in useless portfolio churning if they do not follow their investments closely. A commitment to not change their position for several periods leads to better financial performance. Decision makers can benefit from having a risk policy that they apply routinely to every relevant problem. Examples of such policies are taking the highest deductible when purchasing insurance and never buying extended warranties. A risk policy is a broad frame that aggregates decisions, similar to the outside view of planning problems. This remedy helps combat the biases of excessive optimism and loss aversion. An organization that eliminates both of these biases will perform better.\n\n"
    ],
    "wordCount": 2793
  },
  {
    "tag": "h2",
    "header": "Keeping Score",
    "content": [
      "The chapter discusses how people use money to keep score of their self-regard and achievement. People have different mental accounts for their money, which help them manage and organize their finances, but also lead to irrational decisions. For example, individuals tend to sell winning stocks (Blueberry Tiles) instead of losing stocks (Tiffany Motors), even though selling the losing stock would make more financial sense. This is due to a preference for not adding failure to their record and to the emotional attachment they have with their mental accounts. The same principle applies to companies when they make decisions about investing in a failing project, as executives may prefer to continue with the project despite its poor prospects, in order to avoid personal regret and a permanent stain on their record. The sunk-cost fallacy is a common mistake that keeps people in bad situations, such as unhappy marriages or unpromising projects, due to the fear of regret. However, research suggests that this fallacy can be overcome through education and recognizing it as a mistake.\n\n",
      "The text explores the idea that people's decisions are influenced by the fear of regret. Regret is triggered by alternatives to reality and it is easy to imagine the events that would have been normal under different circumstances. For example, a person who never picks up hitchhikers will experience more regret if they were robbed after picking one up, compared to someone who frequently picks them up. The text also looks at how decision makers anticipate regret, which influences their decisions. For example, consumers are more likely to prefer conventional options when reminded of the possibility of regret. The anticipation of regret also affects life-and-death decisions, such as a physician choosing between treatments for a gravely ill patient. The text touches on the idea that losses are weighted more heavily than gains in several contexts and the reluctance to sell important endowments increases dramatically when doing so might make you responsible for a bad outcome. In the context of parents and their children, the text discusses the aversion to trading increased risk for some other advantage, which is evident in laws and regulations governing risk. The text acknowledges the dilemma between loss-averse moral attitudes and efficient risk management.\n\n",
      "The text focuses on the emotional pains that individuals inflict upon themselves and the impact these intangible outcomes have on their lives. The author notes that while economists are not supposed to have emotions, they can lead to actions that are detrimental to one's wealth, policy, and society. However, the author argues that emotions such as regret and moral responsibility are real and may be relevant in decision-making. The author suggests that explicit anticipation of regret and avoiding hindsight bias can reduce the experience of regret. Additionally, the author cites research that suggests that people generally anticipate more regret than they will actually experience due to the \"psychological immune system.\" The author also highlights some common scenarios where individuals make decisions based on avoiding regret, such as separate mental accounts for cash and credit purchases, the disposition effect, and avoiding trying new things at a restaurant. Finally, the author notes that individuals may feel a taboo tradeoff when making decisions that involve safety and cost.\n\n"
    ],
    "wordCount": 4432
  },
  {
    "tag": "h2",
    "header": "Reversals",
    "content": [
      "The chapter Reversals from Thinking Fast and Slow by Daniel Kahneman discusses how our judgment and choice can be influenced by the context in which they are made, leading to inconsistent results. The author uses two different experiments to illustrate this concept. The first experiment involves compensation for victims of violent crimes. Participants were asked to assign a dollar value to a man who lost the use of his right arm as a result of a gunshot wound. The location of the shooting was varied between two scenarios: the man's regular store and a store he rarely visited. Participants assigned a much higher value to the victim if he was shot in a store he rarely visited than if he was shot in his regular store. This is an example of \"poignancy,\" a feeling evoked by a thought of \"if only\" that translates the strength of the emotional reaction onto a monetary scale. However, when both scenarios were presented together for comparison, almost everyone endorsed the principle that poignancy is not a legitimate consideration.\n\nThe second experiment is a preference reversal between two bets, one safe and one risky. Participants chose the safe bet over the risky one. But when asked to set prices on both bets, they set a higher price on the risky bet than on the safe one. This reversal occurs because joint evaluation focuses attention on an aspect of the situation that was less salient in single evaluation. The emotional reactions of System 1 are more likely to determine single evaluation, while joint evaluation requires a more careful and effortful assessment.\n\nThe author also discusses the impact of these reversals on economics, which was challenged by psychologists' findings. Economists tested thirteen theories and eventually acknowledged that individual choice does depend on the context in which the choices are made, a violation of the coherence doctrine. The author concludes that judgments and preferences are coherent within categories but potentially incoherent when objects belong to different categories.\n\n",
      "People are susceptible to making inconsistent decisions due to their emotional reactions when evaluating issues in single evaluation. This can change when considering the same issues in joint evaluation, which is more likely to produce stable and thoughtful judgments. People can use substitution and intensity matching to make dollar values for a cause, but this can change when considering multiple causes at once. For example, in experiments, people were found to contribute more to saving dolphins than to supporting medical check-ups for farmworkers, but in joint evaluation, they showed a preference for the farmworkers and were willing to contribute more to their welfare. The administration of justice can also be infected by predictable incoherence. Awards to personal injury victims are higher in joint evaluation than in single evaluation, but the legal system favors single evaluation. Fines for violations can vary greatly across agencies and appear odd when compared with each other. Broadening the frame can lead to more reasonable decisions, but one should be wary of deliberate manipulation of the context.\n\n"
    ],
    "wordCount": 3568
  },
  {
    "tag": "h2",
    "header": "Frames and Reality",
    "content": [
      "The chapter discusses the concept of framing and its impact on human decision making. The author argues that humans are not wired to be as rational in their decision making as the concept of an \"Econ\" would suggest. The author uses the example of Italy and France competing in the 2006 World Cup final to demonstrate the difference between logical meaning and the emotional associations that a sentence evokes. The author then introduces the concept of \"framing effects,\" which refers to the unjustified influence of formulation on beliefs and preferences. The author uses the example of a gamble that offers a 10% chance to win $95 and a 90% chance to lose $5 versus a lottery that offers a 10% chance to win $100 and a 90% chance to win nothing to show that the same uncertain prospect can elicit different responses depending on how it is framed. The author also discusses the results of an experiment conducted by neuroscientists at University College London, which combined a study of framing effects with recordings of brain activity. The study found that the amygdala, associated with emotional arousal, was most likely to be active when the subject's choice conformed to the frame. The anterior cingulate, associated with conflict and self-control, was more active when the subject did not follow their natural inclination. The \"rational\" subjects showed enhanced activity in a frontal area of the brain that combines emotion and reasoning to guide decisions. The author also discusses the classic example of emotional framing in medicine, where physician participants were given statistics about two treatments for lung cancer, one of which was framed as survival rates and the other as mortality rates. The author concludes that most people passively accept decision problems as they are framed and that reframing is effortful, so System 2 is normally lazy and does not attempt to simplify the task.\n\n",
      "The text provides insight into how people can be influenced by the way problems are framed. A study is discussed where public-health professionals were given the Asian disease problem in two different formulations, and their answers were swayed by the way the problem was presented to them. The same was true for a class of students at Harvard Kennedy School who were asked about child exemptions in the tax code. The results showed that moral intuitions are attached to frames and descriptions of reality rather than to substance. However, it was noted that some frames are better than others and that broader frames and inclusive accounts lead to more rational decisions. The authors give examples of this, such as the \"fuel economy and environment\" sticker displayed on new cars, which will now include gallons-per-mile information, a directive about organ donation in case of accidental death, and the example of two car owners who seek to reduce their costs by switching cars with different miles per gallon ratings.\n\n",
      "The author explores the concept of human rationality and how it can be swayed by inconsequential factors. An example is given of an organ donation form where people are required to solve a mathematical problem before making their decision. The rate of donations changes dramatically based on the problem given, showing the impact of these inconsequential factors. The debate about human rationality can have real-world consequences, with skeptics being more aware of the impact of these factors compared to those who believe in the rational-agent model. The latter group often overlooks the influence of these factors, leading to inferior outcomes. The author highlights the importance of framing and how it can affect our perception of reality and our emotions. For instance, people feel better if they frame an outcome in terms of money they kept rather than money they lost. The author encourages readers to be sensitive to the power of inconsequential factors and their impact on preferences.\n\n"
    ],
    "wordCount": 4454
  },
  {
    "tag": "h2",
    "header": "Part 5",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Two Selves",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Two Selves",
    "content": [
      "In this chapter, the author focuses on the difference between two concepts of utility - experienced utility and decision utility. Experienced utility refers to the hedonic experiences of pleasure and pain, while decision utility refers to the wantability of a particular outcome. The author discusses the concept of a hedonimeter, a device that would measure the level of pleasure or pain an individual experiences at any moment, and how the total experienced utility can be measured as the area under the curve. The author then introduces the idea of two selves - the experiencing self and the remembering self. The experiencing self is the one that answers the question “Does it hurt now?” while the remembering self is the one that answers the question “How was it, on the whole?” The memories are all that we get to keep from our experience of living, and the only perspective that we can adopt as we think about our lives is therefore that of the remembering self. The author also discusses an experiment that demonstrates the decision-making power of the remembering self, in which participants were asked to hold their hand in cold water for a certain amount of time, and the experimenter recorded the continuous record of the pain they endured. The conclusion is that the actual experience does not always matter, as the memories are what influence our decisions.\n\n",
      "In an experiment, participants were told they would have three cold-hand trials. In reality, they only had two, one short and one long, each with a different hand. Seven minutes after the second trial, participants were asked to choose which episode to repeat for the third trial. The results showed that 80% of participants who reported a decrease in pain intensity during the long trial chose to repeat that episode, despite the fact that it was longer and therefore more painful. This indicates a discrepancy between decision utility and experienced utility, as well as a conflict between the interests of the experiencing self and the remembering self. The cold-hand experiment highlights that our preferences and decisions are shaped by our memories, which can be wrong and inconsistent. The peak-end rule and duration neglect both contribute to this, as they lead to a memory that is strongly influenced by the peak moment and the end of an experience, rather than its overall duration. This leads to decisions that are not always in line with our preferences and interests.\n\n"
    ],
    "wordCount": 3387
  },
  {
    "tag": "h2",
    "header": "Life as a Story",
    "content": [
      "In this chapter, the author reflects on how we view life as a story, and how our memories are influenced by certain events that we remember more than others. The author cites an example of a scene in an opera where the audience is gripped by the tension and fear of whether the young lover will arrive in time to see his beloved before she dies. This highlights how significant events and memorable moments define stories, not just the length of time they take. The author continues to discuss the concept of “duration neglect” and the “peak-end rule” in which evaluations of entire lives, and brief episodes, are determined by their peaks and endings, rather than their overall duration. This idea is supported by experiments conducted by psychologist Ed Diener and his students, where participants were asked to rate the desirability and total happiness of a fictitious character’s life. The results showed that doubling the duration of the character’s life had no effect on the desirability of her life or on judgments of her total happiness. Furthermore, adding five slightly happy years to a very happy life caused a substantial drop in evaluations of the total happiness of that life. The author also reflects on how we choose vacations, highlighting how people tend to evaluate their experiences based on the memories they expect to store. He also provides thought experiments to show that the elimination of memories greatly reduces the value of the experience and that most people feel indifferent to the pains of their experiencing self. The author concludes by discussing how the narrative of a life is often tied to memories, and how Alzheimer’s patients who no longer maintain a narrative of their life are still sensitive to beauty and gentleness.\n\n"
    ],
    "wordCount": 1988
  },
  {
    "tag": "h2",
    "header": "Experienced Well-Being",
    "content": [
      "In the field of well-being, researchers have long relied on survey questions asking people to rate their overall satisfaction with life. The author, however, was suspicious of this approach since he believed that it drew upon the memories of a person's past experiences, not their actual experiences. He therefore proposed to measure happiness objectively by assessing the experiences of a person over successive moments of their life. To do this, he and his team developed a method called the Day Reconstruction Method (DRM), which involved asking participants to relive their previous day in detail, breaking it into episodes, and then answering questions about each episode. The DRM was found to be valid and allowed for the calculation of a duration-weighted measure of an individual's feelings throughout their waking hours. The results of the DRM confirmed that emotions fluctuated over the day and week and were largely determined by what a person was currently attending to. The use of time was one of the areas of life over which people had some control and improvements in transportation, child care, and socializing opportunities could reduce the level of emotional distress in society. National surveys of experienced well-being are now routinely used and have confirmed the importance of situational factors, physical health, and social contact in determining happiness.\n\n",
      "The Cantril Self-Anchoring Striving Scale is used to measure a person's evaluation of their life. The scale asks the person to imagine a ladder with steps numbered from 0 to 10, where the top of the ladder represents the best possible life, and the bottom of the ladder represents the worst possible life. The person is then asked to identify the step on the ladder that represents their personal feelings about their life at that time. Educational attainment has a positive effect on life evaluation, but not on experienced well-being. Ill health has a greater adverse effect on experienced well-being than life evaluation. Living with children has a significant impact on stress and anger, but the adverse effects on life evaluation are smaller. Religion has a greater positive impact on positive affect and stress reduction, but it provides no reduction of depression or worry. An analysis of more than 450,000 responses to the Gallup-Healthways Well-Being Index found that being poor makes one miserable, and being rich may enhance life satisfaction, but it does not improve experienced well-being. Severe poverty intensifies the effects of other misfortunes in life, such as illness, divorce, and loneliness. The weekend has a smaller beneficial impact on experienced well-being for the very poor. A household income of about $75,000 in high-cost areas is the satiation level beyond which experienced well-being no longer increases. The average increase of experienced well-being associated with incomes beyond that level is zero. Higher income is associated with higher satisfaction, well beyond the point at which it ceases to have any positive effect on experience. The difference between life satisfaction and experienced well-being is clear. To increase happiness, it is recommended to control the use of time, and find more time to do things that bring joy. Beyond the satiation level of income, buying more pleasurable experiences may lead to a reduced ability to enjoy less expensive ones.\n\n"
    ],
    "wordCount": 2700
  },
  {
    "tag": "h2",
    "header": "Thinking About Life",
    "content": [
      "The chapter discusses the concept of life satisfaction and happiness. The author argues that people's answers to questions about their life satisfaction are not as simple as they may seem, and that these answers are often influenced by various factors such as current mood, recent events, and goals that people have set for themselves. One example given is a graph that shows a steep decline in life satisfaction after marriage, which the author argues is not necessarily a reflection of unhappiness but could be due to the fact that people tend to think about their recent or future marriage when asked about their life satisfaction. The author also mentions that experienced well-being and life satisfaction are largely determined by genetics and temperament, but can also be influenced by balancing effects where the same situation can be good for some people and bad for others. The author concludes by discussing the focusing illusion, which suggests that people's evaluations of their life are influenced by what they are thinking about at the time.\n\n",
      "A study was conducted to see if people who live in California are happier than others, and to determine the popular beliefs about the happiness of Californians. A survey was conducted with large samples of students from major state universities in California, Ohio, and Michigan. The results showed that the climate did not play a significant role in the life satisfaction of students, and that the students in both regions had the same mistaken view that Californians were happier due to their climate. This error is referred to as the focusing illusion, where too much weight is given to one factor and not enough to all the other determinants of well-being. The focusing illusion can cause people to be wrong about their present state of well-being and the happiness of others. Adaptation to new situations involves thinking less and less about them, and most long-term circumstances of life are part-time states that one inhabits only when they think about them. The study also found that people are prone to miswanting and make bad choices because of errors in affective forecasting. The focusing illusion creates a bias in favor of goods and experiences that are initially exciting, even though they will eventually lose their appeal. The role of time has been a recurring theme in this part of the book, as the mind does not properly represent time and is prone to neglect duration. The mind is good with stories, but not with processing time. The mistake people make in the focusing illusion involves neglecting what happens at other times and focusing on selected moments.\n\n",
      "The text talks about the concept of affective forecasting and the idea that people's expectations about how they'll feel after a certain event might not always align with reality. It mentions examples of people who thought buying a fancy car or a larger house would make them happier, but in reality, it didn't improve their overall well-being as expected. The text also highlights a phenomenon called \"focusing illusion\" and \"miswanting\" where people might be influenced by their current mood or situation when making decisions that impact their long-term happiness. It suggests that people's perceptions of happiness can be subjective and may change depending on various factors.\n\n"
    ],
    "wordCount": 4128
  },
  {
    "tag": "h2",
    "header": "Conclusions",
    "content": null,
    "wordCount": 2
  },
  {
    "tag": "h2",
    "header": "Appendix A: Judgment Under Uncertainty: Heuristics and Biases*",
    "content": null,
    "wordCount": 7
  },
  {
    "tag": "h2",
    "header": "Notes",
    "content": [
      "The author refers to several studies and works done by psychologists and statisticians in the field of human judgment and decision making. The authors D. Kahneman and A. Tversky conducted research on the psychology of prediction and the subjective probability of a person's judgment of representativeness. Another author, W. Edwards, studied conservatism in human information processing. Tversky and Kahneman also looked at the belief in the law of small numbers, and the availability heuristic for judging frequency and probability. Other researchers such as P. Slovic and S. Lichtenstein, M. Bar-Hillel, and J. Cohen, E. I. Chesnick, and D. Haran, conducted studies on topics such as Bayesian and regression approaches to the study of information processing in judgment, the subjective probability of compound events, and the inertial effect in sequential choice and decision. The author also mentions the works of L. J. Savage and B. de Finetti, who have written about the foundations of statistics.\n\n"
    ],
    "wordCount": 315
  },
  {
    "tag": "h2",
    "header": "Appendix B: Choices, Values, And Frames*",
    "content": null,
    "wordCount": 7
  },
  {
    "tag": "h2",
    "header": "Also by Daniel Kahneman",
    "content": null,
    "wordCount": 68
  },
  {
    "tag": "h2",
    "header": "Acknowledgments",
    "content": null,
    "wordCount": 2
  }
]
